{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0221b7e4-76c3-411a-b360-3d550ce27669",
   "metadata": {},
   "source": [
    "# Deploy on AWS Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e700d4-f560-4c53-9758-378f91fe55ac",
   "metadata": {},
   "source": [
    "## Install the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96b24ae9-df68-42f7-8c36-59b436fec448",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T07:41:24.208960Z",
     "iopub.status.busy": "2023-10-16T07:41:24.208542Z",
     "iopub.status.idle": "2023-10-16T07:41:29.850822Z",
     "shell.execute_reply": "2023-10-16T07:41:29.850184Z",
     "shell.execute_reply.started": "2023-10-16T07:41:24.208936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker==2.192.1\n",
      "  Downloading sagemaker-2.192.1.tar.gz (902 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m902.8/902.8 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs<24,>=23.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.131 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (1.28.57)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (1.24.4)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (4.23.4)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (6.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (1.5.3)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (0.3.0)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (0.7.5)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (6.0)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (4.18.4)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (3.9.1)\n",
      "Requirement already satisfied: tblib==1.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (1.7.0)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.57 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker==2.192.1) (1.31.57)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker==2.192.1) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.8.0,>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker==2.192.1) (0.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker==2.192.1) (3.16.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->sagemaker==2.192.1) (3.0.9)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-pasta->sagemaker==2.192.1) (1.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->sagemaker==2.192.1) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->sagemaker==2.192.1) (0.30.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->sagemaker==2.192.1) (0.9.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->sagemaker==2.192.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->sagemaker==2.192.1) (2023.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker==2.192.1) (1.7.6.6)\n",
      "Requirement already satisfied: dill>=0.3.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker==2.192.1) (0.3.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker==2.192.1) (0.3.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker==2.192.1) (0.70.14)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from schema->sagemaker==2.192.1) (21.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore<1.32.0,>=1.31.57->boto3<2.0,>=1.26.131->sagemaker==2.192.1) (1.26.14)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.192.1-py2.py3-none-any.whl size=1206240 sha256=74298756815f019d2b956ac73d58a290b7c775239fe343583b53e59a3b2bc337\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/ba/74/23/fc5c71a7784765c4f3a262981272f28e4b5f812b74d20e1ebe\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.188.0\n",
      "    Uninstalling sagemaker-2.188.0:\n",
      "      Successfully uninstalled sagemaker-2.188.0\n",
      "Successfully installed sagemaker-2.192.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sagemaker==2.192.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e25503-0b58-4ec9-9637-0b2450f2077c",
   "metadata": {},
   "source": [
    "## Deploy the Model as A SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd593ac9-eda4-45d0-ae3c-6660be27a014",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T09:45:51.155017Z",
     "iopub.status.busy": "2023-10-16T09:45:51.154581Z",
     "iopub.status.idle": "2023-10-16T09:45:52.337397Z",
     "shell.execute_reply": "2023-10-16T09:45:52.336821Z",
     "shell.execute_reply.started": "2023-10-16T09:45:51.154996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "import time\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a83494f-6821-4e86-8151-1df6d01e93ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T09:45:54.096821Z",
     "iopub.status.busy": "2023-10-16T09:45:54.096335Z",
     "iopub.status.idle": "2023-10-16T09:45:54.111239Z",
     "shell.execute_reply": "2023-10-16T09:45:54.110666Z",
     "shell.execute_reply.started": "2023-10-16T09:45:54.096802Z"
    }
   },
   "outputs": [],
   "source": [
    "image_uri = get_huggingface_llm_image_uri(\n",
    "  backend=\"huggingface\", # or lmi\n",
    "  region=region,\n",
    " version=\"1.1.0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1f13912-e904-4ef4-8658-6145a2f577be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T09:45:56.365475Z",
     "iopub.status.busy": "2023-10-16T09:45:56.364910Z",
     "iopub.status.idle": "2023-10-16T09:45:56.371204Z",
     "shell.execute_reply": "2023-10-16T09:45:56.370643Z",
     "shell.execute_reply.started": "2023-10-16T09:45:56.365450Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4486b679-f278-4653-bf70-d3fbafdd3780",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:51:17.665047Z",
     "iopub.status.busy": "2023-10-16T10:51:17.664539Z",
     "iopub.status.idle": "2023-10-16T10:59:50.832678Z",
     "shell.execute_reply": "2023-10-16T10:59:50.832149Z",
     "shell.execute_reply.started": "2023-10-16T10:51:17.665029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "----------------!"
     ]
    }
   ],
   "source": [
    "model_name = \"MistralLite-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "hub = {\n",
    "    'HF_MODEL_ID':'amazon/MistralLite',\n",
    "    'HF_TASK':'text-generation',\n",
    "    'SM_NUM_GPUS':'1',\n",
    "    \"MAX_INPUT_LENGTH\": '16000',\n",
    "    \"MAX_TOTAL_TOKENS\": '16384',\n",
    "    \"MAX_BATCH_PREFILL_TOKENS\": '16384',\n",
    "    \"MAX_BATCH_TOTAL_TOKENS\":  '16384',\n",
    "}\n",
    "\n",
    "model = HuggingFaceModel(\n",
    "    name=model_name,\n",
    "    env=hub,\n",
    "    role=role,\n",
    "    image_uri=image_uri\n",
    ")\n",
    "predictor = model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=\"ml.g5.2xlarge\",\n",
    "  endpoint_name=model_name,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d00822-43ea-4117-9e16-a8db5b35179c",
   "metadata": {},
   "source": [
    "## Perform Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f761a5e-e303-40ed-8c23-539a2456a6c9",
   "metadata": {},
   "source": [
    "### Sagemaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d3d5c61-f909-4656-953a-1c5a74bcb91b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:32:05.091305Z",
     "iopub.status.busy": "2023-10-16T10:32:05.090861Z",
     "iopub.status.idle": "2023-10-16T10:32:17.319426Z",
     "shell.execute_reply": "2023-10-16T10:32:17.318777Z",
     "shell.execute_reply.started": "2023-10-16T10:32:05.091287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are several challenges to supporting a long context for LLMs, including:\n",
      "\n",
      "1. Memory Limitations: LLMs are limited in the amount of data they can store in memory, which can limit the length of the context they can support.\n",
      "\n",
      "2. Computational Complexity: Generating responses for long contexts can be computationally expensive, especially for larger models.\n",
      "\n",
      "3. Training Data: Training data for long contexts can be difficult to obtain, as it requires a large amount of text data that is relevant and coherent.\n",
      "\n",
      "4. Bias and Unanswerable Questions: Long contexts can increase the likelihood of bias and unanswerable questions, as the model may not have seen similar contexts during training.\n",
      "\n",
      "5. Incomplete or Inconsistent Data: Long contexts can be more susceptible to errors and inconsistencies in the data, which can affect the accuracy of the model's responses.\n",
      "\n",
      "6. Maintenance and Updating: Maintaining and updating a long context can be challenging, as it requires regular monitoring and updating of the data to ensure its accuracy and relevance.\n",
      "\n",
      "7. User Interface: Supporting a long context may require a more complex user interface, which can be difficult to design and implement.\n",
      "\n",
      "8. Scalability: As the context grows, the model may become more difficult to scale, which can affect its performance and efficiency.\n",
      "\n",
      "9. Security and Privacy: Long contexts may raise concerns about security and privacy, as they may contain sensitive information that needs to be protected.\n",
      "\n",
      "10. Legal and Ethical Considerations: Long contexts may raise legal and ethical concerns, such as liability for the model's responses and the potential for bias in the data.\n"
     ]
    }
   ],
   "source": [
    "input_data = {\n",
    "  \"inputs\": \"<|prompter|>What are the main challenges to support a long context for LLM?</s><|assistant|>\",\n",
    "  \"parameters\": {\n",
    "    \"do_sample\": False,\n",
    "    \"max_new_tokens\": 400,\n",
    "    \"return_full_text\": False,\n",
    "    #\"typical_p\": 0.2,\n",
    "    #\"temperature\":None,\n",
    "    #\"truncate\":None,\n",
    "    #\"seed\": 1,\n",
    "  }\n",
    "}\n",
    "result = predictor.predict(input_data)[0][\"generated_text\"]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ef69aa-62ab-426f-81f5-8eff78d672b7",
   "metadata": {},
   "source": [
    "### boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db1c124e-2a40-4525-b06b-8ead20348016",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T11:02:28.467617Z",
     "iopub.status.busy": "2023-10-16T11:02:28.467180Z",
     "iopub.status.idle": "2023-10-16T11:02:40.779537Z",
     "shell.execute_reply": "2023-10-16T11:02:40.778964Z",
     "shell.execute_reply.started": "2023-10-16T11:02:28.467600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are several challenges to supporting a long context for LLMs, including:\n",
      "\n",
      "1. Memory Limitations: LLMs are limited in the amount of data they can store in memory, which can limit the length of the context they can support.\n",
      "\n",
      "2. Computational Complexity: Generating responses for long contexts can be computationally expensive, especially for larger models.\n",
      "\n",
      "3. Training Data: Training data for long contexts can be difficult to obtain, as it requires a large amount of text data that is relevant and coherent.\n",
      "\n",
      "4. Bias and Unanswerable Questions: Long contexts can increase the likelihood of bias and unanswerable questions, as the model may not have seen similar contexts during training.\n",
      "\n",
      "5. Incomplete or Inconsistent Data: Long contexts can be more susceptible to errors and inconsistencies in the data, which can affect the accuracy of the model's responses.\n",
      "\n",
      "6. Maintenance and Updating: Maintaining and updating a long context can be challenging, as it requires regular monitoring and updating of the data to ensure its accuracy and relevance.\n",
      "\n",
      "7. User Interface: Supporting a long context may require a more complex user interface, which can be difficult to design and implement.\n",
      "\n",
      "8. Scalability: As the context grows, the model may become more difficult to scale, which can affect its performance and efficiency.\n",
      "\n",
      "9. Security and Privacy: Long contexts may raise concerns about security and privacy, as they may contain sensitive information that needs to be protected.\n",
      "\n",
      "10. Legal and Ethical Considerations: Long contexts may raise legal and ethical concerns, such as liability for the model's responses and the potential for bias in the data.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "def call_endpoint(client, prompt, endpoint_name, paramters):\n",
    "    client = boto3.client(\"sagemaker-runtime\")\n",
    "    payload = {\"inputs\": prompt,\n",
    "               \"parameters\": parameters}\n",
    "    response = client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                      Body=json.dumps(payload), \n",
    "                                      ContentType=\"application/json\")\n",
    "    output = json.loads(response[\"Body\"].read().decode())\n",
    "    result = output[0][\"generated_text\"]\n",
    "    return result\n",
    "\n",
    "client = boto3.client(\"sagemaker-runtime\")\n",
    "parameters = {\n",
    "    \"do_sample\": False,\n",
    "    \"max_new_tokens\": 400,\n",
    "    \"return_full_text\": False,\n",
    "    #\"typical_p\": 0.2,\n",
    "    #\"temperature\":None,\n",
    "    #\"truncate\":None,\n",
    "    #\"seed\": 1,\n",
    "}\n",
    "endpoint_name = predictor.endpoint_name\n",
    "prompt = \"<|prompter|>What are the main challenges to support a long context for LLM?</s><|assistant|>\"\n",
    "result = call_endpoint(client, prompt, endpoint_name, parameters)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ebd431a9-af16-409a-b69d-4b554dd84ac7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T11:02:44.417099Z",
     "iopub.status.busy": "2023-10-16T11:02:44.416611Z",
     "iopub.status.idle": "2023-10-16T11:02:49.073290Z",
     "shell.execute_reply": "2023-10-16T11:02:49.072551Z",
     "shell.execute_reply.started": "2023-10-16T11:02:44.417081Z"
    }
   },
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (424) from primary with message \"{\"error\":\"Request failed during generation: Server error: CUDA error: invalid argument\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\",\"error_type\":\"generation\"}\". See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/MistralLite-2023-10-16-10-51-17 in account 161422014849 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m     task_instruction \u001b[38;5;241m=\u001b[39m task_instruction\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m      4\u001b[0m         my_question\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease tell me how does pgvector help with Generative AI and give me some examples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m     )\n\u001b[1;32m      6\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|prompter|>\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_instruction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m</s><|assistant|>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mcall_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[0;32mIn[30], line 7\u001b[0m, in \u001b[0;36mcall_endpoint\u001b[0;34m(client, prompt, endpoint_name, paramters)\u001b[0m\n\u001b[1;32m      4\u001b[0m client \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mclient(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msagemaker-runtime\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[1;32m      6\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m: parameters}\n\u001b[0;32m----> 7\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEndpointName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mBody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mContentType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m output \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mdecode())\n\u001b[1;32m     11\u001b[0m result \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/client.py:535\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m     )\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/client.py:980\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    978\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m parsed_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    979\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 980\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (424) from primary with message \"{\"error\":\"Request failed during generation: Server error: CUDA error: invalid argument\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\",\"error_type\":\"generation\"}\". See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/MistralLite-2023-10-16-10-51-17 in account 161422014849 for more information."
     ]
    }
   ],
   "source": [
    "with open(\"../example_long_ctx.txt\", \"r\") as fin:\n",
    "    task_instruction = fin.read()\n",
    "    task_instruction = task_instruction.format(\n",
    "        my_question=\"please tell me how does pgvector help with Generative AI and give me some examples.\"\n",
    "    )\n",
    "prompt = f\"<|prompter|>{task_instruction}</s><|assistant|>\"\n",
    "result = call_endpoint(client, prompt, endpoint_name, parameters)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
