{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0221b7e4-76c3-411a-b360-3d550ce27669",
   "metadata": {},
   "source": [
    "# Deploy on AWS Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e700d4-f560-4c53-9758-378f91fe55ac",
   "metadata": {},
   "source": [
    "## Install the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b24ae9-df68-42f7-8c36-59b436fec448",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker==2.192.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e25503-0b58-4ec9-9637-0b2450f2077c",
   "metadata": {},
   "source": [
    "## Deploy the Model as A SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd593ac9-eda4-45d0-ae3c-6660be27a014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "import time\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a83494f-6821-4e86-8151-1df6d01e93ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = get_huggingface_llm_image_uri(\n",
    "  backend=\"huggingface\", # or lmi\n",
    "  region=region,\n",
    " version=\"1.1.0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f13912-e904-4ef4-8658-6145a2f577be",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4486b679-f278-4653-bf70-d3fbafdd3780",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"MistralLite-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "hub = {\n",
    "    'HF_MODEL_ID':'amazon/MistralLite',\n",
    "    'HF_TASK':'text-generation',\n",
    "    'SM_NUM_GPUS':'1',\n",
    "    \"MAX_INPUT_LENGTH\": '12000',\n",
    "    \"MAX_TOTAL_TOKENS\": '12288',\n",
    "    \"MAX_BATCH_PREFILL_TOKENS\": '12288',\n",
    "    \"MAX_BATCH_TOTAL_TOKENS\":  '12288',\n",
    "    \"DTYPE\": 'bfloat16',\n",
    "}\n",
    "\n",
    "model = HuggingFaceModel(\n",
    "    name=model_name,\n",
    "    env=hub,\n",
    "    role=role,\n",
    "    image_uri=image_uri\n",
    ")\n",
    "predictor = model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=\"ml.g5.2xlarge\",\n",
    "  endpoint_name=model_name,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d00822-43ea-4117-9e16-a8db5b35179c",
   "metadata": {},
   "source": [
    "## Perform Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f761a5e-e303-40ed-8c23-539a2456a6c9",
   "metadata": {},
   "source": [
    "### Sagemaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3d5c61-f909-4656-953a-1c5a74bcb91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\n",
    "  \"inputs\": \"<|prompter|>What are the main challenges to support a long context for LLM?</s><|assistant|>\",\n",
    "  \"parameters\": {\n",
    "    \"do_sample\": False,\n",
    "    \"max_new_tokens\": 400,\n",
    "    \"return_full_text\": False,\n",
    "    #\"typical_p\": 0.2,\n",
    "    #\"temperature\":None,\n",
    "    #\"truncate\":None,\n",
    "    #\"seed\": 1,\n",
    "  }\n",
    "}\n",
    "result = predictor.predict(input_data)[0][\"generated_text\"]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ef69aa-62ab-426f-81f5-8eff78d672b7",
   "metadata": {},
   "source": [
    "### boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1c124e-2a40-4525-b06b-8ead20348016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "def call_endpoint(client, prompt, endpoint_name, paramters):\n",
    "    client = boto3.client(\"sagemaker-runtime\")\n",
    "    payload = {\"inputs\": prompt,\n",
    "               \"parameters\": parameters}\n",
    "    response = client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                      Body=json.dumps(payload), \n",
    "                                      ContentType=\"application/json\")\n",
    "    output = json.loads(response[\"Body\"].read().decode())\n",
    "    result = output[0][\"generated_text\"]\n",
    "    return result\n",
    "\n",
    "client = boto3.client(\"sagemaker-runtime\")\n",
    "parameters = {\n",
    "    \"do_sample\": False,\n",
    "    \"max_new_tokens\": 400,\n",
    "    \"return_full_text\": False,\n",
    "    #\"typical_p\": 0.2,\n",
    "    #\"temperature\":None,\n",
    "    #\"truncate\":None,\n",
    "    #\"seed\": 10,\n",
    "}\n",
    "endpoint_name = predictor.endpoint_name\n",
    "prompt = \"<|prompter|>What are the main challenges to support a long context for LLM?</s><|assistant|>\"\n",
    "result = call_endpoint(client, prompt, endpoint_name, parameters)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da523da-0d51-4228-9152-bca8c0ccfd1e",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd65c98-a8da-40af-948d-e2fd23b33bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "class LineIterator:\n",
    "    \"\"\"\n",
    "    A helper class for parsing the byte stream input. \n",
    "    \n",
    "    The output of the model will be in the following format:\n",
    "    ```\n",
    "    b'{\"outputs\": [\" a\"]}\\n'\n",
    "    b'{\"outputs\": [\" challenging\"]}\\n'\n",
    "    b'{\"outputs\": [\" problem\"]}\\n'\n",
    "    ...\n",
    "    ```\n",
    "    \n",
    "    While usually each PayloadPart event from the event stream will contain a byte array \n",
    "    with a full json, this is not guaranteed and some of the json objects may be split across\n",
    "    PayloadPart events. For example:\n",
    "    ```\n",
    "    {'PayloadPart': {'Bytes': b'{\"outputs\": '}}\n",
    "    {'PayloadPart': {'Bytes': b'[\" problem\"]}\\n'}}\n",
    "    ```\n",
    "    \n",
    "    This class accounts for this by concatenating bytes written via the 'write' function\n",
    "    and then exposing a method which will return lines (ending with a '\\n' character) within\n",
    "    the buffer via the 'scan_lines' function. It maintains the position of the last read \n",
    "    position to ensure that previous bytes are not exposed again. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stream):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "            if line and line[-1] == ord('\\n'):\n",
    "                self.read_pos += len(line)\n",
    "                return line[:-1]\n",
    "            try:\n",
    "                chunk = next(self.byte_iterator)\n",
    "            except StopIteration:\n",
    "                if self.read_pos < self.buffer.getbuffer().nbytes:\n",
    "                    continue\n",
    "                raise\n",
    "            if 'PayloadPart' not in chunk:\n",
    "                print('Unknown event type:' + chunk)\n",
    "                continue\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk['PayloadPart']['Bytes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73155e3d-91b2-4cdb-9548-fd29fbd60b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_endpoint_streaming(client, prompt, endpoint_name, paramters):\n",
    "    client = boto3.client(\"sagemaker-runtime\")\n",
    "    payload = {\"inputs\": prompt,\n",
    "               \"parameters\": parameters,\n",
    "               \"stream\": True}\n",
    "    response = client.invoke_endpoint_with_response_stream(EndpointName=endpoint_name,\n",
    "                                                           Body=json.dumps(payload),\n",
    "                                                           ContentType='application/json')\n",
    "    output = \"\"\n",
    "    event_stream = response['Body']\n",
    "    start_json = b'{'\n",
    "    for line in LineIterator(event_stream):\n",
    "        if line != b'' and start_json in line:\n",
    "            data = json.loads(line[line.find(start_json):].decode('utf-8'))\n",
    "            if not data['token'][\"special\"]:\n",
    "                print(data['token']['text'],end='')\n",
    "                output += data['token']['text']\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b400ad-4fb3-4f64-abd2-77ba9ace3ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client(\"sagemaker-runtime\")\n",
    "parameters = {\n",
    "    \"do_sample\": False,\n",
    "    \"max_new_tokens\": 400,\n",
    "    \"return_full_text\": False,\n",
    "    #\"typical_p\": 0.2,\n",
    "    #\"temperature\":None,\n",
    "    #\"truncate\":None,\n",
    "    #\"seed\": 10,\n",
    "}\n",
    "endpoint_name = predictor.endpoint_name\n",
    "prompt = \"<|prompter|>What are the main challenges to support a long context for LLM?</s><|assistant|>\"\n",
    "result = call_endpoint_streaming(client, prompt, endpoint_name, parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
