{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2111eee8",
   "metadata": {},
   "source": [
    "# Using MistralLite with HuggingFace Transformers and FlashAttention-2\n",
    "\n",
    "This notebook provides a step-by-step walkthrough of using the open source MistralLite model for natural language generation with HuggingFace's transformers library and flash attention mechanisms. We will initialize the model using Hugging Face's transformers library, and leverage flash attention to enable stronger performance on long context tasks. The notebook includes code to load the model, create a text generation pipeline, test it on short prompts, and demonstrate its capabilities on longer prompts from files. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da36e624-f632-46ec-9e79-f4176e73a386",
   "metadata": {},
   "source": [
    "## Installing packages for local deployment\n",
    "The following cells install the necessary libraries to run MistralLite on a SageMaker notebook instance, using a ml.g5.2xlarge instance type. Execute the following cells to install the packages and restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989afeee-34a3-4122-be24-be2f91983145",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.34.0 accelerate==0.23.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8ce453",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flash-attn==2.3.1.post1 --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578e336b-8be9-4578-98cc-c83a89ff5bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "restartkernel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c934fec-48d2-4f18-867e-ce9283984f6f",
   "metadata": {},
   "source": [
    "## Deploying MistralLite using a Transformers Pipeline\n",
    "\n",
    "Execute the following cell to load the pretrained MistralLite model and tokenizer, and create a text generation pipeline using them. The pipeline handles prompt encoding and generating text. This cell may take a few minutes to execute to fully load and initialize the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9c8042-090b-40ff-906e-46ec26c6206a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"amazon/MistralLite\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             use_flash_attention_2=True,\n",
    "                                             device_map=\"auto\",)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc5756a3",
   "metadata": {},
   "source": [
    "## Question-Answering Examples\n",
    "\n",
    "The following two cells contain example queries to the deployed LLM, showcasing a simple question-answering prompt and a context-aware prompt from Amazon Aurora documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0433b09-052b-47cb-a552-075522e4c846",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = \"<|prompter|>What are the main challenges to support a long context for LLM?</s><|assistant|>\"\n",
    "\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=400,\n",
    "    do_sample=False,\n",
    "    return_full_text=False,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1437661-8d06-43a1-9547-ffdea1529d3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"../example_long_ctx.txt\", \"r\", encoding=\"utf-8\") as fin:\n",
    "    task_instruction = fin.read()\n",
    "    task_instruction = task_instruction.format(\n",
    "        my_question=\"please tell me how does pgvector help with Generative AI and give me some examples.\"\n",
    "    )\n",
    "prompt = f\"<|prompter|>{task_instruction}</s><|assistant|>\"\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=400,\n",
    "    do_sample=False,\n",
    "    return_full_text=False,\n",
    "    num_return_sequences=1,\n",
    "    #eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
