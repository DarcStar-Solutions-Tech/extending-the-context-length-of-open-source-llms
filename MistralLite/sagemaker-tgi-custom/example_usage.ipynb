{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Long Context MistralLite on SageMaker Endpoints\n",
    "\n",
    "This notebook provides a step-by-step walkthrough of deploying the open source MistralLite model for **long context** natural language generation with SageMaker. We will build the custom container for long-context inference, deploy the LLM as a SageMaker Endpoint, and invoke the deployed endpoint with example prompts. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "For this example we will use the [SageMaker SDK](https://sagemaker.readthedocs.io/en/stable/) to create and deploy the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker==2.192.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Custom Container\n",
    "First we augment the Dockerfile for SageMaker. Execute the following cells to add additional commands to the Dockerfile, and build the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_entry_stmt = \"\"\"\n",
    "# Text Generation Inference base env\n",
    "ENV HUGGINGFACE_HUB_CACHE=/tmp \\\n",
    "    HF_HUB_ENABLE_HF_TRANSFER=1 \\\n",
    "    PORT=80\n",
    "COPY sagemaker-entrypoint.sh entrypoint.sh\n",
    "RUN chmod +x entrypoint.sh\n",
    "\n",
    "ENTRYPOINT [\"./entrypoint.sh\"]\n",
    "CMD [ \"\" ]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../tgi-custom/Dockerfile\", \"r\") as fin:\n",
    "    docker_content = fin.read()\n",
    "\n",
    "sm_docker_cotent = docker_content + sm_entry_stmt\n",
    "\n",
    "with open(\"Dockerfile_sm\", \"w\") as fout:\n",
    "    fout.write(sm_docker_cotent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we build the image. This could take 10 minutes, so feel free to run it directly in the terminal in case the notebook cell times out.  \n",
    "\n",
    "**Important Note** - Please ensure the `ROLE` has sufficient permission to push Docker images to Elastic Container Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r ../tgi-custom/vllm ./vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_NAME = \"mistrallite-tgi110-ecr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!bash sm_build.sh {REPO_NAME}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy SageMaker Endpoint\n",
    "The SageMaker SDK provides support to deploy open-source LLMs with just a few lines of code, powered by [HuggingFace's Text Generation Inference container](https://github.com/huggingface/text-generation-inference). Execute the next set of cells to deploy a long-context-enabled MistralLite on a `ml.g5.2xlarge` real-time inference endpoint on SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker\n",
    "import time\n",
    "\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "\n",
    "def get_aws_region():\n",
    "    # Get the current AWS region from the default session\n",
    "    session = boto3.session.Session()\n",
    "    return session.region_name\n",
    "\n",
    "def get_aws_account_id():\n",
    "    # Get the current AWS account ID from the default session\n",
    "    sts_client = boto3.client(\"sts\")\n",
    "    response = sts_client.get_caller_identity()\n",
    "    return response[\"Account\"]\n",
    "\n",
    "REGION = get_aws_region()\n",
    "ACCOUNT_ID = get_aws_account_id()\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = f\"{ACCOUNT_ID}.dkr.ecr.{REGION}.amazonaws.com/{REPO_NAME}\"\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"MistralLite-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "num_gpu = 1\n",
    "max_input_length = 24000\n",
    "max_total_tokens = 24576\n",
    "\n",
    "hub = {\n",
    "    'HF_MODEL_ID':'amazon/MistralLite',\n",
    "    'HF_TASK':'text-generation',\n",
    "    'SM_NUM_GPUS': json.dumps(num_gpu),\n",
    "    \"MAX_INPUT_LENGTH\": json.dumps(max_input_length),\n",
    "    \"MAX_TOTAL_TOKENS\": json.dumps(max_total_tokens),\n",
    "    \"MAX_BATCH_PREFILL_TOKENS\": json.dumps(max_total_tokens),\n",
    "    \"MAX_BATCH_TOTAL_TOKENS\":  json.dumps(max_total_tokens),\n",
    "    \"DTYPE\": 'bfloat16',\n",
    "}\n",
    "\n",
    "model = HuggingFaceModel(\n",
    "    name=model_name,\n",
    "    env=hub,\n",
    "    role=role,\n",
    "    image_uri=image_uri\n",
    ")\n",
    "\n",
    "print(\"☕ Spinning up the endpoint. This will take a little while ☕\")\n",
    "\n",
    "predictor = model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  endpoint_name=model_name,   \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Inference\n",
    "\n",
    "There are a couple ways you can invoke the deployed model, either through the SageMaker SDK or through the AWS SDK for Python, [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html). Both methods are provided as examples in the following cells."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Inference via Sagemaker SDK\n",
    "Execute the following cell to invoke the deployed LLM endpoint on a sample prompt using the SageMaker SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\n",
    "  \"inputs\": \"<|prompter|>What are the main challenges to support a long context for LLM?</s><|assistant|>\",\n",
    "  \"parameters\": {\n",
    "    \"do_sample\": False,\n",
    "    \"max_new_tokens\": 400,\n",
    "    \"return_full_text\": False,\n",
    "    #\"typical_p\": 0.2,\n",
    "    #\"temperature\":None,\n",
    "    #\"truncate\":None,\n",
    "    #\"seed\": 1,\n",
    "  }\n",
    "}\n",
    "result = predictor.predict(input_data)[0][\"generated_text\"]\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Inference via boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_endpoint(client, prompt, endpoint_name, paramters):\n",
    "    client = boto3.client(\"sagemaker-runtime\")\n",
    "    payload = {\"inputs\": prompt,\n",
    "               \"parameters\": parameters}\n",
    "    response = client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                      Body=json.dumps(payload), \n",
    "                                      ContentType=\"application/json\")\n",
    "    output = json.loads(response[\"Body\"].read().decode())\n",
    "    result = output[0][\"generated_text\"]\n",
    "    return result\n",
    "\n",
    "client = boto3.client(\"sagemaker-runtime\")\n",
    "parameters = {\n",
    "    \"do_sample\": False,\n",
    "    \"max_new_tokens\": 400,\n",
    "    \"return_full_text\": False,\n",
    "    #\"typical_p\": 0.2,\n",
    "    #\"temperature\":None,\n",
    "    #\"truncate\":None,\n",
    "    #\"seed\": 10,\n",
    "}\n",
    "endpoint_name = predictor.endpoint_name\n",
    "prompt = \"<|prompter|>What are the main challenges to support a long context for LLM?</s><|assistant|>\"\n",
    "result = call_endpoint(client, prompt, endpoint_name, parameters)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long-Context Inference with Boto3\n",
    "\n",
    "With boto3, try the long context of over 13,400 tokens, which are copied from [Amazon Aurora FAQs](https://aws.amazon.com/rds/aurora/faqs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../example_long_ctx.txt\", \"r\") as fin:\n",
    "    task_instruction = fin.read()\n",
    "    task_instruction = task_instruction.format(\n",
    "        my_question=\"please tell me how does pgvector help with Generative AI and give me some examples.\"\n",
    "    )\n",
    "prompt = f\"<|prompter|>{task_instruction}</s><|assistant|>\"\n",
    "result = call_endpoint(client, prompt, endpoint_name, parameters)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Responses\n",
    "\n",
    "To stream the response, execute the following cells to create a LineIterator class and a streaming response invocation function that will provide a seamless streamed response from the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "class LineIterator:\n",
    "    \"\"\"\n",
    "    A helper class for parsing the byte stream input. \n",
    "    \n",
    "    The output of the model will be in the following format:\n",
    "    ```\n",
    "    b'{\"outputs\": [\" a\"]}\\n'\n",
    "    b'{\"outputs\": [\" challenging\"]}\\n'\n",
    "    b'{\"outputs\": [\" problem\"]}\\n'\n",
    "    ...\n",
    "    ```\n",
    "    \n",
    "    While usually each PayloadPart event from the event stream will contain a byte array \n",
    "    with a full json, this is not guaranteed and some of the json objects may be split across\n",
    "    PayloadPart events. For example:\n",
    "    ```\n",
    "    {'PayloadPart': {'Bytes': b'{\"outputs\": '}}\n",
    "    {'PayloadPart': {'Bytes': b'[\" problem\"]}\\n'}}\n",
    "    ```\n",
    "    \n",
    "    This class accounts for this by concatenating bytes written via the 'write' function\n",
    "    and then exposing a method which will return lines (ending with a '\\n' character) within\n",
    "    the buffer via the 'scan_lines' function. It maintains the position of the last read \n",
    "    position to ensure that previous bytes are not exposed again. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stream):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "            if line and line[-1] == ord('\\n'):\n",
    "                self.read_pos += len(line)\n",
    "                return line[:-1]\n",
    "            try:\n",
    "                chunk = next(self.byte_iterator)\n",
    "            except StopIteration:\n",
    "                if self.read_pos < self.buffer.getbuffer().nbytes:\n",
    "                    continue\n",
    "                raise\n",
    "            if 'PayloadPart' not in chunk:\n",
    "                print('Unknown event type:' + chunk)\n",
    "                continue\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk['PayloadPart']['Bytes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_endpoint_streaming(client, prompt, endpoint_name, paramters):\n",
    "    client = boto3.client(\"sagemaker-runtime\")\n",
    "    payload = {\"inputs\": prompt,\n",
    "               \"parameters\": parameters,\n",
    "               \"stream\": True}\n",
    "    response = client.invoke_endpoint_with_response_stream(EndpointName=endpoint_name,\n",
    "                                                           Body=json.dumps(payload),\n",
    "                                                           ContentType='application/json')\n",
    "    output = \"\"\n",
    "    event_stream = response['Body']\n",
    "    start_json = b'{'\n",
    "    for line in LineIterator(event_stream):\n",
    "        if line != b'' and start_json in line:\n",
    "            data = json.loads(line[line.find(start_json):].decode('utf-8'))\n",
    "            if not data['token'][\"special\"]:\n",
    "                print(data['token']['text'],end='')\n",
    "                output += data['token']['text']\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client(\"sagemaker-runtime\")\n",
    "parameters = {\n",
    "    \"do_sample\": False,\n",
    "    \"max_new_tokens\": 400,\n",
    "    \"return_full_text\": False,\n",
    "    #\"typical_p\": 0.2,\n",
    "    #\"temperature\":None,\n",
    "    #\"truncate\":None,\n",
    "    #\"seed\": 10,\n",
    "}\n",
    "endpoint_name = predictor.endpoint_name\n",
    "prompt = \"<|prompter|>What are the main challenges to support a long context for LLM?</s><|assistant|>\"\n",
    "result = call_endpoint_streaming(client, prompt, endpoint_name, parameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long-Context Streaming\n",
    "With the new class and function, try the long context of over 13,400 tokens, which are copied from [Amazon Aurora FAQs](https://aws.amazon.com/rds/aurora/faqs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../example_long_ctx.txt\", \"r\") as fin:\n",
    "    task_instruction = fin.read()\n",
    "    task_instruction = task_instruction.format(\n",
    "        my_question=\"please tell me how does pgvector help with Generative AI and give me some examples.\"\n",
    "    )\n",
    "prompt = f\"<|prompter|>{task_instruction}</s><|assistant|>\"\n",
    "result = call_endpoint_streaming(client, prompt, endpoint_name, parameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "After you've finished using the endpoint, it's important to delete it to avoid incurring unnecessary costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
