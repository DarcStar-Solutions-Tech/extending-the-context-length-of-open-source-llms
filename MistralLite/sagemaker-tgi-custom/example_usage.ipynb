{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker==2.192.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Custom Container\n",
    "First we augment the Dockerfile for SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_entry_stmt = \"\"\"\n",
    "# Text Generation Inference base env\n",
    "ENV HUGGINGFACE_HUB_CACHE=/tmp \\\n",
    "    HF_HUB_ENABLE_HF_TRANSFER=1 \\\n",
    "    PORT=80\n",
    "COPY sagemaker-entrypoint.sh entrypoint.sh\n",
    "RUN chmod +x entrypoint.sh\n",
    "\n",
    "ENTRYPOINT [\"./entrypoint.sh\"]\n",
    "CMD [ \"\" ]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../tgi-custom/Dockerfile\", \"r\") as fin:\n",
    "    docker_content = fin.read()\n",
    "\n",
    "sm_docker_cotent = docker_content + sm_entry_stmt\n",
    "\n",
    "with open(\"Dockerfile_sm\", \"w\") as fout:\n",
    "    fout.write(sm_docker_cotent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we build the image. This could take 10 minutes, so feel free to run it directly in the terminal in case the notebook cell times out.  \n",
    "\n",
    "**Important Note** - Please ensure the `ROLE` has sufficient permission to push Docker images to Elastic Container Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r ../tgi-custom/vllm ./vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_NAME = \"mistrallite-tgi110-ecr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!bash sm_build.sh {REPO_NAME}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker\n",
    "import time\n",
    "\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "\n",
    "def get_aws_region():\n",
    "    # Get the current AWS region from the default session\n",
    "    session = boto3.session.Session()\n",
    "    return session.region_name\n",
    "\n",
    "def get_aws_account_id():\n",
    "    # Get the current AWS account ID from the default session\n",
    "    sts_client = boto3.client(\"sts\")\n",
    "    response = sts_client.get_caller_identity()\n",
    "    return response[\"Account\"]\n",
    "\n",
    "REGION = get_aws_region()\n",
    "ACCOUNT_ID = get_aws_account_id()\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = f\"{ACCOUNT_ID}.dkr.ecr.{REGION}.amazonaws.com/{REPO_NAME}\"\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"MistralLite-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "num_gpu = 1\n",
    "max_input_length = 24000\n",
    "max_total_tokens = 24576\n",
    "\n",
    "hub = {\n",
    "    'HF_MODEL_ID':'amazon/MistralLite',\n",
    "    'HF_TASK':'text-generation',\n",
    "    'SM_NUM_GPUS': json.dumps(num_gpu),\n",
    "    \"MAX_INPUT_LENGTH\": json.dumps(max_input_length),\n",
    "    \"MAX_TOTAL_TOKENS\": json.dumps(max_total_tokens),\n",
    "    \"MAX_BATCH_PREFILL_TOKENS\": json.dumps(max_total_tokens),\n",
    "    \"MAX_BATCH_TOTAL_TOKENS\":  json.dumps(max_total_tokens),\n",
    "    \"DTYPE\": 'bfloat16',\n",
    "}\n",
    "\n",
    "model = HuggingFaceModel(\n",
    "    name=model_name,\n",
    "    env=hub,\n",
    "    role=role,\n",
    "    image_uri=image_uri\n",
    ")\n",
    "predictor = model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  endpoint_name=model_name,   \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sagemaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\n",
    "  \"inputs\": \"<|prompter|>What are the main challenges to support a long context for LLM?</s><|assistant|>\",\n",
    "  \"parameters\": {\n",
    "    \"do_sample\": False,\n",
    "    \"max_new_tokens\": 400,\n",
    "    \"return_full_text\": False,\n",
    "    #\"typical_p\": 0.2,\n",
    "    #\"temperature\":None,\n",
    "    #\"truncate\":None,\n",
    "    #\"seed\": 1,\n",
    "  }\n",
    "}\n",
    "result = predictor.predict(input_data)[0][\"generated_text\"]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "def call_endpoint(client, prompt, endpoint_name, paramters):\n",
    "    client = boto3.client(\"sagemaker-runtime\")\n",
    "    payload = {\"inputs\": prompt,\n",
    "               \"parameters\": parameters}\n",
    "    response = client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                      Body=json.dumps(payload), \n",
    "                                      ContentType=\"application/json\")\n",
    "    output = json.loads(response[\"Body\"].read().decode())\n",
    "    result = output[0][\"generated_text\"]\n",
    "    return result\n",
    "\n",
    "client = boto3.client(\"sagemaker-runtime\")\n",
    "parameters = {\n",
    "    \"do_sample\": False,\n",
    "    \"max_new_tokens\": 400,\n",
    "    \"return_full_text\": False,\n",
    "    #\"typical_p\": 0.2,\n",
    "    #\"temperature\":None,\n",
    "    #\"truncate\":None,\n",
    "    #\"seed\": 10,\n",
    "}\n",
    "endpoint_name = predictor.endpoint_name\n",
    "prompt = \"<|prompter|>What are the main challenges to support a long context for LLM?</s><|assistant|>\"\n",
    "result = call_endpoint(client, prompt, endpoint_name, parameters)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the long context of over 13,400 tokens, which are copied from [Amazon Aurora FAQs](https://aws.amazon.com/rds/aurora/faqs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../example_long_ctx.txt\", \"r\") as fin:\n",
    "    task_instruction = fin.read()\n",
    "    task_instruction = task_instruction.format(\n",
    "        my_question=\"please tell me how does pgvector help with Generative AI and give me some examples.\"\n",
    "    )\n",
    "prompt = f\"<|prompter|>{task_instruction}</s><|assistant|>\"\n",
    "result = call_endpoint(client, prompt, endpoint_name, parameters)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
