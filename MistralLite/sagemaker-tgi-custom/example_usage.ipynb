{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-17T04:25:56.403319Z",
     "iopub.status.busy": "2023-10-17T04:25:56.402898Z",
     "iopub.status.idle": "2023-10-17T04:25:59.223669Z",
     "shell.execute_reply": "2023-10-17T04:25:59.223009Z",
     "shell.execute_reply.started": "2023-10-17T04:25:56.403301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker==2.192.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.192.1)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.131 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (1.28.57)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (1.24.4)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (4.23.4)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (6.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (1.5.3)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (0.3.0)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (0.7.5)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (6.0)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (4.18.4)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (3.9.1)\n",
      "Requirement already satisfied: tblib==1.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker==2.192.1) (1.7.0)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.57 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker==2.192.1) (1.31.57)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker==2.192.1) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.8.0,>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker==2.192.1) (0.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker==2.192.1) (3.16.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->sagemaker==2.192.1) (3.0.9)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-pasta->sagemaker==2.192.1) (1.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->sagemaker==2.192.1) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->sagemaker==2.192.1) (0.30.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->sagemaker==2.192.1) (0.9.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->sagemaker==2.192.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->sagemaker==2.192.1) (2023.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker==2.192.1) (1.7.6.6)\n",
      "Requirement already satisfied: dill>=0.3.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker==2.192.1) (0.3.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker==2.192.1) (0.3.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker==2.192.1) (0.70.14)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from schema->sagemaker==2.192.1) (21.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore<1.32.0,>=1.31.57->boto3<2.0,>=1.26.131->sagemaker==2.192.1) (1.26.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sagemaker==2.192.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Custom Container\n",
    "First we augment the Dockerfile for SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-17T04:56:47.932359Z",
     "iopub.status.busy": "2023-10-17T04:56:47.931988Z",
     "iopub.status.idle": "2023-10-17T04:56:47.935572Z",
     "shell.execute_reply": "2023-10-17T04:56:47.934808Z",
     "shell.execute_reply.started": "2023-10-17T04:56:47.932341Z"
    }
   },
   "outputs": [],
   "source": [
    "sm_entry_stmt = \"\"\"\n",
    "# Text Generation Inference base env\n",
    "ENV HUGGINGFACE_HUB_CACHE=/tmp \\\n",
    "    HF_HUB_ENABLE_HF_TRANSFER=1 \\\n",
    "    PORT=80\n",
    "COPY sagemaker-entrypoint.sh entrypoint.sh\n",
    "RUN chmod +x entrypoint.sh\n",
    "\n",
    "ENTRYPOINT [\"./entrypoint.sh\"]\n",
    "CMD [ \"\" ]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-17T04:56:50.625643Z",
     "iopub.status.busy": "2023-10-17T04:56:50.625228Z",
     "iopub.status.idle": "2023-10-17T04:56:50.629125Z",
     "shell.execute_reply": "2023-10-17T04:56:50.628603Z",
     "shell.execute_reply.started": "2023-10-17T04:56:50.625625Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"../tgi-custom/Dockerfile\", \"r\") as fin:\n",
    "    docker_content = fin.read()\n",
    "\n",
    "sm_docker_cotent = docker_content + sm_entry_stmt\n",
    "\n",
    "with open(\"Dockerfile_sm\", \"w\") as fout:\n",
    "    fout.write(sm_docker_cotent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we build the image. This could take 10 minutes, so feel free to run it directly in the terminal in case the notebook cell times out.  \n",
    "\n",
    "**Important Note** - Please ensure the `ROLE` has sufficient permission to push Docker images to Elastic Container Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-17T05:14:32.033653Z",
     "iopub.status.busy": "2023-10-17T05:14:32.033280Z",
     "iopub.status.idle": "2023-10-17T05:14:32.148810Z",
     "shell.execute_reply": "2023-10-17T05:14:32.148182Z",
     "shell.execute_reply.started": "2023-10-17T05:14:32.033631Z"
    }
   },
   "outputs": [],
   "source": [
    "!cp -r ../tgi-custom/vllm ./vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-17T04:56:56.435589Z",
     "iopub.status.busy": "2023-10-17T04:56:56.435052Z",
     "iopub.status.idle": "2023-10-17T04:56:56.438584Z",
     "shell.execute_reply": "2023-10-17T04:56:56.437907Z",
     "shell.execute_reply.started": "2023-10-17T04:56:56.435571Z"
    }
   },
   "outputs": [],
   "source": [
    "REPO_NAME = \"mistrallite-tgi110-ecr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-17T04:56:58.649981Z",
     "iopub.status.busy": "2023-10-17T04:56:58.649546Z",
     "iopub.status.idle": "2023-10-17T04:57:04.397038Z",
     "shell.execute_reply": "2023-10-17T04:57:04.396272Z",
     "shell.execute_reply.started": "2023-10-17T04:56:58.649963Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO_NAME: mistrallite-tgi110-ecr\n",
      "REGION: us-west-2\n",
      "ACCOUNT_ID: 161422014849\n",
      "1.1.0: Pulling from huggingface/text-generation-inference\n",
      "Digest: sha256:553a1397345e37bc5c6c9eb2300ee46c7dd5c55937c245871608f903f7f71d7d\n",
      "Status: Image is up to date for ghcr.io/huggingface/text-generation-inference:1.1.0\n",
      "ghcr.io/huggingface/text-generation-inference:1.1.0\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "{\n",
      "    \"repositories\": [\n",
      "        {\n",
      "            \"repositoryArn\": \"arn:aws:ecr:us-west-2:161422014849:repository/mistrallite-tgi110-ecr\",\n",
      "            \"registryId\": \"161422014849\",\n",
      "            \"repositoryName\": \"mistrallite-tgi110-ecr\",\n",
      "            \"repositoryUri\": \"161422014849.dkr.ecr.us-west-2.amazonaws.com/mistrallite-tgi110-ecr\",\n",
      "            \"createdAt\": 1697516781.0,\n",
      "            \"imageTagMutability\": \"MUTABLE\",\n",
      "            \"imageScanningConfiguration\": {\n",
      "                \"scanOnPush\": false\n",
      "            },\n",
      "            \"encryptionConfiguration\": {\n",
      "                \"encryptionType\": \"AES256\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "Sending build context to Docker daemon  190.5kB\n",
      "Step 1/33 : FROM debian:bullseye-slim as pytorch-install\n",
      " ---> 1411dd05edfc\n",
      "Step 2/33 : ARG PYTORCH_VERSION=2.0.1\n",
      " ---> Using cache\n",
      " ---> ce0b1c250647\n",
      "Step 3/33 : ARG PYTHON_VERSION=3.9\n",
      " ---> Using cache\n",
      " ---> 31613d3e3dae\n",
      "Step 4/33 : ARG CUDA_VERSION=11.8\n",
      " ---> Using cache\n",
      " ---> a6c754f560da\n",
      "Step 5/33 : ARG MAMBA_VERSION=23.1.0-1\n",
      " ---> Using cache\n",
      " ---> 595317c72a6c\n",
      "Step 6/33 : ARG CUDA_CHANNEL=nvidia\n",
      " ---> Using cache\n",
      " ---> 4f0012d80a92\n",
      "Step 7/33 : ARG INSTALL_CHANNEL=pytorch\n",
      " ---> Using cache\n",
      " ---> 424e31a0e8cb\n",
      "Step 8/33 : ARG TARGETPLATFORM\n",
      " ---> Using cache\n",
      " ---> 23a82f9ff2ee\n",
      "Step 9/33 : ENV PATH /opt/conda/bin:$PATH\n",
      " ---> Using cache\n",
      " ---> 951b4363dcfa\n",
      "Step 10/33 : RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends         build-essential         ca-certificates         ccache         curl         git &&         rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 858c5d97d790\n",
      "Step 11/33 : RUN case ${TARGETPLATFORM} in          \"linux/arm64\")  MAMBA_ARCH=aarch64  ;;          *)              MAMBA_ARCH=x86_64   ;;     esac &&     curl -fsSL -v -o ~/mambaforge.sh -O  \"https://github.com/conda-forge/miniforge/releases/download/${MAMBA_VERSION}/Mambaforge-${MAMBA_VERSION}-Linux-${MAMBA_ARCH}.sh\"\n",
      " ---> Using cache\n",
      " ---> 43dacd9d332e\n",
      "Step 12/33 : RUN chmod +x ~/mambaforge.sh &&     bash ~/mambaforge.sh -b -p /opt/conda &&     rm ~/mambaforge.sh\n",
      " ---> Using cache\n",
      " ---> 26daf7dc76e9\n",
      "Step 13/33 : RUN case ${TARGETPLATFORM} in          \"linux/arm64\")  exit 1 ;;          *)              /opt/conda/bin/conda update -y conda &&                           /opt/conda/bin/conda install -c \"${INSTALL_CHANNEL}\" -c \"${CUDA_CHANNEL}\" -y \"python=${PYTHON_VERSION}\" pytorch==$PYTORCH_VERSION \"pytorch-cuda=$(echo $CUDA_VERSION | cut -d'.' -f 1-2)\"  ;;     esac &&     /opt/conda/bin/conda clean -ya\n",
      " ---> Using cache\n",
      " ---> fe87f251fc19\n",
      "Step 14/33 : FROM pytorch-install as kernel-builder\n",
      " ---> fe87f251fc19\n",
      "Step 15/33 : RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends         ninja-build         && rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> ad4b2c983215\n",
      "Step 16/33 : RUN /opt/conda/bin/conda install -c \"nvidia/label/cuda-11.8.0\"  cuda==11.8 &&     /opt/conda/bin/conda clean -ya\n",
      " ---> Using cache\n",
      " ---> a56f5e441b5d\n",
      "Step 17/33 : WORKDIR /usr/src\n",
      " ---> Using cache\n",
      " ---> 6b104a92b656\n",
      "Step 18/33 : RUN git clone https://github.com/OlivierDehaene/vllm.git\n",
      " ---> Using cache\n",
      " ---> 4c33e3c1a088\n",
      "Step 19/33 : WORKDIR /usr/src/vllm\n",
      " ---> Using cache\n",
      " ---> 88fd283fb0c7\n",
      "Step 20/33 : RUN git fetch\n",
      " ---> Using cache\n",
      " ---> 158977d4f6d6\n",
      "Step 21/33 : RUN git checkout d284b831c17f42a8ea63369a06138325f73c4cf9\n",
      " ---> Using cache\n",
      " ---> 02b486518eb9\n",
      "Step 22/33 : COPY vllm/attention_kernels.cu /usr/src/vllm/csrc/attention/attention_kernels.cu\n",
      " ---> Using cache\n",
      " ---> c681f3c3d740\n",
      "Step 23/33 : COPY vllm/test_attention.py /usr/src/vllm/tests/kernels/test_attention.py\n",
      " ---> Using cache\n",
      " ---> b4fb1983fab3\n",
      "Step 24/33 : COPY vllm//utils.py  /usr/src/vllm/vllm/utils.py\n",
      " ---> Using cache\n",
      " ---> a8957bec0db8\n",
      "Step 25/33 : COPY vllm/worker.py /usr/src/vllm/vllm/worker/worker.py\n",
      " ---> Using cache\n",
      " ---> 2616ada3da62\n",
      "Step 26/33 : RUN python setup.py build\n",
      " ---> Using cache\n",
      " ---> 8c273a923ae3\n",
      "Step 27/33 : FROM ghcr.io/huggingface/text-generation-inference:1.1.0\n",
      " ---> a111faa3a21b\n",
      "Step 28/33 : COPY --from=kernel-builder /usr/src/vllm/build/lib.linux-x86_64-cpython-39 /opt/conda/lib/python3.9/site-packages\n",
      " ---> Using cache\n",
      " ---> 7340c352527c\n",
      "Step 29/33 : ENV HUGGINGFACE_HUB_CACHE=/tmp     HF_HUB_ENABLE_HF_TRANSFER=1     PORT=80\n",
      " ---> Running in 5cf07c94e9e6\n",
      "Removing intermediate container 5cf07c94e9e6\n",
      " ---> 1e9352fdd03f\n",
      "Step 30/33 : COPY sagemaker-entrypoint.sh entrypoint.sh\n",
      " ---> 01f5c6b81120\n",
      "Step 31/33 : RUN chmod +x entrypoint.sh\n",
      " ---> Running in 001590a2394a\n",
      "Removing intermediate container 001590a2394a\n",
      " ---> 0d18d83db9d9\n",
      "Step 32/33 : ENTRYPOINT [\"./entrypoint.sh\"]\n",
      " ---> Running in bcf7c549daa8\n",
      "Removing intermediate container bcf7c549daa8\n",
      " ---> fbb1d49ddc22\n",
      "Step 33/33 : CMD [ \"\" ]\n",
      " ---> Running in 2096f2cd3d27\n",
      "Removing intermediate container 2096f2cd3d27\n",
      " ---> 8222052ba8bf\n",
      "Successfully built 8222052ba8bf\n",
      "Successfully tagged mistrallite-tgi110-ecr:latest\n",
      "The push refers to repository [161422014849.dkr.ecr.us-west-2.amazonaws.com/mistrallite-tgi110-ecr]\n",
      "\n",
      "\u001b[1Bbc5a1dc7: Preparing \n",
      "\u001b[1Bf0bb7bc7: Preparing \n",
      "\u001b[1B3cbea645: Preparing \n",
      "\u001b[1B2f380aef: Preparing \n",
      "\u001b[1Bd1f899f1: Preparing \n",
      "\u001b[1B11787bb5: Preparing \n",
      "\u001b[1Bbbf73d6f: Preparing \n",
      "\u001b[1Bdafcd59e: Preparing \n",
      "\u001b[1Bbf18a086: Preparing \n",
      "\u001b[1B6f7d5cf3: Preparing \n",
      "\u001b[1Bb8eccc59: Preparing \n",
      "\u001b[1Be25943d7: Preparing \n",
      "\u001b[1B73d1acba: Preparing \n",
      "\u001b[1Bfbf3a462: Preparing \n",
      "\u001b[1B3d378079: Preparing \n",
      "\u001b[1B42227215: Preparing \n",
      "\u001b[1B707181e3: Preparing \n",
      "\u001b[1B0682bd59: Preparing \n",
      "\u001b[1Bd16899b8: Preparing \n",
      "\u001b[1Bd22eac67: Preparing \n",
      "\u001b[1Ba2300eef: Preparing \n",
      "\u001b[14Bf18a086: Preparing \n",
      "\u001b[18B1787bb5: Waiting g \n",
      "\u001b[1Bb4e1ecd1: Preparing \n",
      "\u001b[1B5c845fcf: Preparing \n",
      "\u001b[1Ba7216f78: Preparing \n",
      "\u001b[1Bd8cea54a: Layer already exists \u001b[22A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[26A\u001b[2K\u001b[27A\u001b[2K\u001b[2A\u001b[2Klatest: digest: sha256:e376901705c3753823a506653c71a7896c0ca849e432f042bed93f7fb3f8fc50 size: 6207\n",
      "Container URI:\n",
      "161422014849.dkr.ecr.us-west-2.amazonaws.com/mistrallite-tgi110-ecr:latest\n"
     ]
    }
   ],
   "source": [
    "!bash sm_build.sh {REPO_NAME}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-17T04:57:09.839371Z",
     "iopub.status.busy": "2023-10-17T04:57:09.838837Z",
     "iopub.status.idle": "2023-10-17T04:57:10.229904Z",
     "shell.execute_reply": "2023-10-17T04:57:10.229221Z",
     "shell.execute_reply.started": "2023-10-17T04:57:09.839351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker\n",
    "import time\n",
    "\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "\n",
    "def get_aws_region():\n",
    "    # Get the current AWS region from the default session\n",
    "    session = boto3.session.Session()\n",
    "    return session.region_name\n",
    "\n",
    "def get_aws_account_id():\n",
    "    # Get the current AWS account ID from the default session\n",
    "    sts_client = boto3.client(\"sts\")\n",
    "    response = sts_client.get_caller_identity()\n",
    "    return response[\"Account\"]\n",
    "\n",
    "REGION = get_aws_region()\n",
    "ACCOUNT_ID = get_aws_account_id()\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-17T04:57:12.080473Z",
     "iopub.status.busy": "2023-10-17T04:57:12.080012Z",
     "iopub.status.idle": "2023-10-17T04:57:12.084572Z",
     "shell.execute_reply": "2023-10-17T04:57:12.083955Z",
     "shell.execute_reply.started": "2023-10-17T04:57:12.080455Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'161422014849.dkr.ecr.us-west-2.amazonaws.com/mistrallite-tgi110-ecr'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_uri = f\"{ACCOUNT_ID}.dkr.ecr.{REGION}.amazonaws.com/{REPO_NAME}\"\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-17T04:57:21.730577Z",
     "iopub.status.busy": "2023-10-17T04:57:21.730163Z",
     "iopub.status.idle": "2023-10-17T05:05:54.814450Z",
     "shell.execute_reply": "2023-10-17T05:05:54.813921Z",
     "shell.execute_reply.started": "2023-10-17T04:57:21.730559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "----------------!"
     ]
    }
   ],
   "source": [
    "model_name = \"MistralLite-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "num_gpu = 1\n",
    "max_input_length = 24000\n",
    "max_total_tokens = 24576\n",
    "\n",
    "hub = {\n",
    "    'HF_MODEL_ID':'amazon/MistralLite',\n",
    "    'HF_TASK':'text-generation',\n",
    "    'SM_NUM_GPUS': json.dumps(num_gpu),\n",
    "    \"MAX_INPUT_LENGTH\": json.dumps(max_input_length),\n",
    "    \"MAX_TOTAL_TOKENS\": json.dumps(max_total_tokens),\n",
    "    \"MAX_BATCH_PREFILL_TOKENS\": json.dumps(max_total_tokens),\n",
    "    \"MAX_BATCH_TOTAL_TOKENS\":  json.dumps(max_total_tokens),\n",
    "    \"DTYPE\": 'bfloat16',\n",
    "}\n",
    "\n",
    "model = HuggingFaceModel(\n",
    "    name=model_name,\n",
    "    env=hub,\n",
    "    role=role,\n",
    "    image_uri=image_uri\n",
    ")\n",
    "predictor = model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  endpoint_name=model_name,   \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sagemaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-17T05:05:54.815672Z",
     "iopub.status.busy": "2023-10-17T05:05:54.815490Z",
     "iopub.status.idle": "2023-10-17T05:06:07.231044Z",
     "shell.execute_reply": "2023-10-17T05:06:07.230454Z",
     "shell.execute_reply.started": "2023-10-17T05:05:54.815656Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are several challenges to supporting a long context for LLMs, including:\n",
      "\n",
      "1. Memory Limitations: LLMs are limited in the amount of data they can store in memory, which can limit the length of the context they can support.\n",
      "\n",
      "2. Computational Complexity: Generating a response for a long context can be computationally expensive, especially for larger models.\n",
      "\n",
      "3. Training Data: Training an LLM to support a long context requires a large amount of training data, which can be difficult and time-consuming to collect.\n",
      "\n",
      "4. Bias and Unanswerable Questions: As the context gets longer, there is a greater risk of bias and unanswerable questions, which can be difficult for the model to handle.\n",
      "\n",
      "5. Human Evaluation: Evaluating the quality of responses for a long context can be challenging, as it may require human evaluators to read and understand a large amount of text.\n",
      "\n",
      "6. Scalability: As the context gets longer, it becomes more difficult to scale the model to handle the increased computational demands.\n",
      "\n",
      "7. Interpretability: It can be difficult to understand how an LLM arrives at its responses for a long context, making it challenging to identify and address any biases or errors in the model.\n",
      "\n",
      "8. Cost: Supporting a long context can be expensive, as it requires more computational resources and training data.\n",
      "\n",
      "9. Security and Privacy: As the context gets longer, there is a greater risk of security and privacy concerns, as the model may be exposed to sensitive information.\n",
      "\n",
      "10. Legal and Ethical Considerations: Supporting a long context may raise legal and ethical concerns, such as the potential for the model to generate biased or harmful responses.\n"
     ]
    }
   ],
   "source": [
    "input_data = {\n",
    "  \"inputs\": \"<|prompter|>What are the main challenges to support a long context for LLM?</s><|assistant|>\",\n",
    "  \"parameters\": {\n",
    "    \"do_sample\": False,\n",
    "    \"max_new_tokens\": 400,\n",
    "    \"return_full_text\": False,\n",
    "    #\"typical_p\": 0.2,\n",
    "    #\"temperature\":None,\n",
    "    #\"truncate\":None,\n",
    "    #\"seed\": 1,\n",
    "  }\n",
    "}\n",
    "result = predictor.predict(input_data)[0][\"generated_text\"]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-17T05:06:07.232336Z",
     "iopub.status.busy": "2023-10-17T05:06:07.231907Z",
     "iopub.status.idle": "2023-10-17T05:06:19.578763Z",
     "shell.execute_reply": "2023-10-17T05:06:19.578184Z",
     "shell.execute_reply.started": "2023-10-17T05:06:07.232312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are several challenges to supporting a long context for LLMs, including:\n",
      "\n",
      "1. Memory Limitations: LLMs are limited in the amount of data they can store in memory, which can limit the length of the context they can support.\n",
      "\n",
      "2. Computational Complexity: Generating a response for a long context can be computationally expensive, especially for larger models.\n",
      "\n",
      "3. Training Data: Training an LLM to support a long context requires a large amount of training data, which can be difficult and time-consuming to collect.\n",
      "\n",
      "4. Bias and Unanswerable Questions: As the context gets longer, there is a greater risk of bias and unanswerable questions, which can be difficult for the model to handle.\n",
      "\n",
      "5. Human Evaluation: Evaluating the quality of responses for a long context can be challenging, as it may require human evaluators to read and understand a large amount of text.\n",
      "\n",
      "6. Scalability: As the context gets longer, it becomes more difficult to scale the model to handle the increased computational demands.\n",
      "\n",
      "7. Interpretability: It can be difficult to understand how an LLM arrives at its responses for a long context, making it challenging to identify and address any biases or errors in the model.\n",
      "\n",
      "8. Cost: Supporting a long context can be expensive, as it requires more computational resources and training data.\n",
      "\n",
      "9. Security and Privacy: As the context gets longer, there is a greater risk of security and privacy concerns, as the model may be exposed to sensitive information.\n",
      "\n",
      "10. Legal and Ethical Considerations: Supporting a long context may raise legal and ethical concerns, such as the potential for the model to generate biased or harmful responses.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "def call_endpoint(client, prompt, endpoint_name, paramters):\n",
    "    client = boto3.client(\"sagemaker-runtime\")\n",
    "    payload = {\"inputs\": prompt,\n",
    "               \"parameters\": parameters}\n",
    "    response = client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                      Body=json.dumps(payload), \n",
    "                                      ContentType=\"application/json\")\n",
    "    output = json.loads(response[\"Body\"].read().decode())\n",
    "    result = output[0][\"generated_text\"]\n",
    "    return result\n",
    "\n",
    "client = boto3.client(\"sagemaker-runtime\")\n",
    "parameters = {\n",
    "    \"do_sample\": False,\n",
    "    \"max_new_tokens\": 400,\n",
    "    \"return_full_text\": False,\n",
    "    #\"typical_p\": 0.2,\n",
    "    #\"temperature\":None,\n",
    "    #\"truncate\":None,\n",
    "    #\"seed\": 10,\n",
    "}\n",
    "endpoint_name = predictor.endpoint_name\n",
    "prompt = \"<|prompter|>What are the main challenges to support a long context for LLM?</s><|assistant|>\"\n",
    "result = call_endpoint(client, prompt, endpoint_name, parameters)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the long context of over 13,400 tokens, which are copied from [Amazon Aurora FAQs](https://aws.amazon.com/rds/aurora/faqs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-17T05:06:19.580628Z",
     "iopub.status.busy": "2023-10-17T05:06:19.580170Z",
     "iopub.status.idle": "2023-10-17T05:06:41.487544Z",
     "shell.execute_reply": "2023-10-17T05:06:41.486866Z",
     "shell.execute_reply.started": "2023-10-17T05:06:19.580611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pgvector is an open-source extension for PostgreSQL supported by Amazon Aurora PostgreSQL-Compatible Edition.\n",
      "\n",
      "You can use pgvector to store, search, index, and query billions of embeddings that are generated from machine learning (ML) and artificial intelligence (AI) models in your database, such as those from Amazon Bedrock (limited preview) or Amazon SageMaker. A vector embedding is a numerical representation that represents the semantic meaning of content such as text, images, and video.\n",
      "\n",
      "With pgvector, you can query embeddings in your Aurora PostgreSQL database to perform efficient semantic similarity searches of these data types, represented as vectors, combined with other tabular data in Aurora. This enables the use of generative AI and other AI/ML systems for new types of applications such as personalized recommendations based on similar text descriptions or images, candidate match based on interview notes, customer service next best action recommendations based on successful transcripts or chat session dialogs, and more.\n"
     ]
    }
   ],
   "source": [
    "with open(\"../example_long_ctx.txt\", \"r\") as fin:\n",
    "    task_instruction = fin.read()\n",
    "    task_instruction = task_instruction.format(\n",
    "        my_question=\"please tell me how does pgvector help with Generative AI and give me some examples.\"\n",
    "    )\n",
    "prompt = f\"<|prompter|>{task_instruction}</s><|assistant|>\"\n",
    "result = call_endpoint(client, prompt, endpoint_name, parameters)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
