{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87c4bfe1-272c-4189-bd13-a73aa9f81475",
   "metadata": {},
   "source": [
    "# How to Serve MistralFlite on TGI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8144f6b-d4cf-4868-92ba-a9d959fb5ad2",
   "metadata": {},
   "source": [
    "## Start TGI Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89423632-5115-46e8-83ce-310be9dc4fa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T05:32:05.144808Z",
     "iopub.status.busy": "2023-10-16T05:32:05.144564Z",
     "iopub.status.idle": "2023-10-16T05:32:05.255145Z",
     "shell.execute_reply": "2023-10-16T05:32:05.254455Z",
     "shell.execute_reply.started": "2023-10-16T05:32:05.144788Z"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a07043b7-43c3-41a3-a9e0-3049834b32ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T07:29:08.993384Z",
     "iopub.status.busy": "2023-10-16T07:29:08.992902Z",
     "iopub.status.idle": "2023-10-16T07:29:10.237994Z",
     "shell.execute_reply": "2023-10-16T07:29:10.237346Z",
     "shell.execute_reply.started": "2023-10-16T07:29:08.993363Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5cb854d3699438d50ab9ee41cd49e67da1366455a4c8a3ae4e47e41ffa440468\n"
     ]
    }
   ],
   "source": [
    "!docker run -d --gpus all --shm-size 1g -p 443:80 -v $(pwd)/models:/data ghcr.io/huggingface/text-generation-inference:1.1.0 \\\n",
    "      --model-id amazon/MistralLite \\\n",
    "      --max-input-length 8192 \\\n",
    "      --max-total-tokens 16384 \\\n",
    "      --max-batch-prefill-tokens 16384 \\\n",
    "      --trust-remote-code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b15f0e-926f-4c3d-b648-d1de85b0a82b",
   "metadata": {},
   "source": [
    "> Warning: You may need to wait for 10+ minutes for the docker container to be ready for the first time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57d026d-c5e1-45d9-8104-adb70f9ca890",
   "metadata": {},
   "source": [
    "## Perform Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65a91fc8-3b33-47a0-98c3-95415168b9ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T04:52:08.159446Z",
     "iopub.status.busy": "2023-10-16T04:52:08.159015Z",
     "iopub.status.idle": "2023-10-16T04:52:12.969501Z",
     "shell.execute_reply": "2023-10-16T04:52:12.968896Z",
     "shell.execute_reply.started": "2023-10-16T04:52:08.159427Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting text_generation\n",
      "  Obtaining dependency information for text_generation from https://files.pythonhosted.org/packages/14/f7/cadf3a0fc619a72d7c667d16e96ef0a5b4c557e6e2b4788a0360dfba4fee/text_generation-0.6.1-py3-none-any.whl.metadata\n",
      "  Downloading text_generation-0.6.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting aiohttp<4.0,>=3.8 (from text_generation)\n",
      "  Obtaining dependency information for aiohttp<4.0,>=3.8 from https://files.pythonhosted.org/packages/41/8e/4c48881316bbced3d13089c4d0df4be321ce79a0c695d82dee9996aaf56b/aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from text_generation) (0.16.4)\n",
      "Requirement already satisfied: pydantic<3,>1.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from text_generation) (2.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->text_generation) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->text_generation) (3.2.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0,>=3.8->text_generation)\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp<4.0,>=3.8->text_generation)\n",
      "  Obtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0,>=3.8->text_generation)\n",
      "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp<4.0,>=3.8->text_generation)\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/1e/28/74b8b6451c89c070d34e753d8b65a1e4ce508a6808b18529f36e8c0e2184/frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0,>=3.8->text_generation)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.12->text_generation) (3.12.2)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.12->text_generation) (2023.6.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.12->text_generation) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.12->text_generation) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.12->text_generation) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.12->text_generation) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.12->text_generation) (21.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3,>1.10->text_generation) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3,>1.10->text_generation) (2.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub<1.0,>=0.12->text_generation) (3.0.9)\n",
      "Requirement already satisfied: idna>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp<4.0,>=3.8->text_generation) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.12->text_generation) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.12->text_generation) (2023.5.7)\n",
      "Downloading text_generation-0.6.1-py3-none-any.whl (10 kB)\n",
      "Downloading aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.7/225.7 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, text_generation\n",
      "Successfully installed aiohttp-3.8.6 aiosignal-1.3.1 async-timeout-4.0.3 frozenlist-1.4.0 multidict-6.0.4 text_generation-0.6.1 yarl-1.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip install text_generation==0.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2a88eb8e-2872-4065-a851-1f3fa7048f77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T07:37:15.278244Z",
     "iopub.status.busy": "2023-10-16T07:37:15.277851Z",
     "iopub.status.idle": "2023-10-16T07:37:15.283080Z",
     "shell.execute_reply": "2023-10-16T07:37:15.282375Z",
     "shell.execute_reply.started": "2023-10-16T07:37:15.278226Z"
    }
   },
   "outputs": [],
   "source": [
    "from text_generation import Client\n",
    "\n",
    "SERVER_PORT = 443\n",
    "SERVER_HOST = \"localhost\"\n",
    "SERVER_URL = f\"{SERVER_HOST}:{SERVER_PORT}\"\n",
    "tgi_client = Client(f\"http://{SERVER_URL}\", timeout=60)\n",
    "\n",
    "def invoke_tgi(prompt, \n",
    "                      random_seed=1, \n",
    "                      max_new_tokens=400, \n",
    "                      print_stream=True,\n",
    "                      assist_role=True):\n",
    "    if (assist_role):\n",
    "        prompt = f\"<|prompter|>{prompt}<|/s|><|assistant|>\"\n",
    "    output = \"\"\n",
    "    for response in tgi_client.generate_stream(\n",
    "        prompt,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=None,\n",
    "        truncate=None,\n",
    "        seed=random_seed,\n",
    "        typical_p=0.2,\n",
    "    ):\n",
    "        if hasattr(response, \"token\"):\n",
    "            if not response.token.special:\n",
    "                snippet = response.token.text\n",
    "                output += snippet\n",
    "                if (print_stream):\n",
    "                    print(snippet, end='', flush=True)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "68cd018a-ae4d-42ce-8dc0-b77d53afd92a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T07:37:17.542747Z",
     "iopub.status.busy": "2023-10-16T07:37:17.542364Z",
     "iopub.status.idle": "2023-10-16T07:37:29.710508Z",
     "shell.execute_reply": "2023-10-16T07:37:29.709904Z",
     "shell.execute_reply.started": "2023-10-16T07:37:17.542729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1.  Access to sufficient training data\n",
      "    To support a long context for LLM, the model needs to be trained on a large and diverse set of data that captures a wide range of language phenomena and concepts.  However, acquiring and annotating this data can be time-consuming and costly, and may require specialized knowledge or resources.\n",
      "    2.  Data imbalance and bias\n",
      "    The data used to train the model may not accurately reflect the real-world distribution of language use, and may be biased towards certain demographic groups or topics.  This can result in the model producing biased or inaccurate outputs.\n",
      "    3.  Memory limitations\n",
      "    The amount of memory required to store and process a long context can be prohibitive, especially for larger models.  This can limit the effectiveness of the model in handling complex or nuanced language phenomena.\n",
      "    4.  Training time and computational resources\n",
      "    Training a model that can handle a long context can take a significant amount of time and computational resources, especially for larger models.  This can be a challenge for organizations with limited budgets or resources.\n",
      "    5.  Complexity of language phenomena\n",
      "    Capturing and modeling complex and nuanced language phenomena can be challenging, even for the most advanced LLMs.  This can limit the effectiveness of the model in handling certain types of inputs or outputs.\n",
      "    6.  Model interpretation and explainability\n",
      "    Interpreting and explaining the behavior of LLMs that process long contexts can be difficult, especially for models that use deep neural networks.  This can limit the effectiveness of the model in certain applications where transparency and explainability are required."
     ]
    }
   ],
   "source": [
    "prompt = \"What are the main challenges to support a long context for LLM?\"\n",
    "result = invoke_tgi(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
