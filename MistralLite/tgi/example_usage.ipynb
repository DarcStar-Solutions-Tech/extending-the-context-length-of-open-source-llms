{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87c4bfe1-272c-4189-bd13-a73aa9f81475",
   "metadata": {},
   "source": [
    "# How to Serve MistralFlite on TGI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8144f6b-d4cf-4868-92ba-a9d959fb5ad2",
   "metadata": {},
   "source": [
    "## Start TGI Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "89423632-5115-46e8-83ce-310be9dc4fa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:52:20.634946Z",
     "iopub.status.busy": "2023-10-16T10:52:20.634595Z",
     "iopub.status.idle": "2023-10-16T10:52:20.747824Z",
     "shell.execute_reply": "2023-10-16T10:52:20.747019Z",
     "shell.execute_reply.started": "2023-10-16T10:52:20.634928Z"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a07043b7-43c3-41a3-a9e0-3049834b32ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:52:23.200025Z",
     "iopub.status.busy": "2023-10-16T10:52:23.199631Z",
     "iopub.status.idle": "2023-10-16T10:52:24.468083Z",
     "shell.execute_reply": "2023-10-16T10:52:24.467229Z",
     "shell.execute_reply.started": "2023-10-16T10:52:23.200004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2a0cfd4e6a78231f9811edef810ee3a0360cae774a04ec56bb6073b742687bed\n"
     ]
    }
   ],
   "source": [
    "!docker run -d --gpus all --shm-size 1g -p 443:80 -v $(pwd)/models:/data ghcr.io/huggingface/text-generation-inference:1.1.0 \\\n",
    "      --model-id amazon/MistralLite \\\n",
    "      --max-input-length 16000 \\\n",
    "      --max-total-tokens 16384 \\\n",
    "      --max-batch-prefill-tokens 16384 \\\n",
    "      --trust-remote-code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b15f0e-926f-4c3d-b648-d1de85b0a82b",
   "metadata": {},
   "source": [
    "> Warning: You may need to wait for 10+ minutes for the docker container to be ready for the first time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57d026d-c5e1-45d9-8104-adb70f9ca890",
   "metadata": {},
   "source": [
    "## Perform Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65a91fc8-3b33-47a0-98c3-95415168b9ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T04:52:08.159446Z",
     "iopub.status.busy": "2023-10-16T04:52:08.159015Z",
     "iopub.status.idle": "2023-10-16T04:52:12.969501Z",
     "shell.execute_reply": "2023-10-16T04:52:12.968896Z",
     "shell.execute_reply.started": "2023-10-16T04:52:08.159427Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting text_generation\n",
      "  Obtaining dependency information for text_generation from https://files.pythonhosted.org/packages/14/f7/cadf3a0fc619a72d7c667d16e96ef0a5b4c557e6e2b4788a0360dfba4fee/text_generation-0.6.1-py3-none-any.whl.metadata\n",
      "  Downloading text_generation-0.6.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting aiohttp<4.0,>=3.8 (from text_generation)\n",
      "  Obtaining dependency information for aiohttp<4.0,>=3.8 from https://files.pythonhosted.org/packages/41/8e/4c48881316bbced3d13089c4d0df4be321ce79a0c695d82dee9996aaf56b/aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from text_generation) (0.16.4)\n",
      "Requirement already satisfied: pydantic<3,>1.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from text_generation) (2.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->text_generation) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->text_generation) (3.2.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0,>=3.8->text_generation)\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp<4.0,>=3.8->text_generation)\n",
      "  Obtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0,>=3.8->text_generation)\n",
      "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp<4.0,>=3.8->text_generation)\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/1e/28/74b8b6451c89c070d34e753d8b65a1e4ce508a6808b18529f36e8c0e2184/frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0,>=3.8->text_generation)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.12->text_generation) (3.12.2)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.12->text_generation) (2023.6.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.12->text_generation) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.12->text_generation) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.12->text_generation) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.12->text_generation) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.12->text_generation) (21.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3,>1.10->text_generation) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3,>1.10->text_generation) (2.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub<1.0,>=0.12->text_generation) (3.0.9)\n",
      "Requirement already satisfied: idna>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp<4.0,>=3.8->text_generation) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.12->text_generation) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.12->text_generation) (2023.5.7)\n",
      "Downloading text_generation-0.6.1-py3-none-any.whl (10 kB)\n",
      "Downloading aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.7/225.7 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, text_generation\n",
      "Successfully installed aiohttp-3.8.6 aiosignal-1.3.1 async-timeout-4.0.3 frozenlist-1.4.0 multidict-6.0.4 text_generation-0.6.1 yarl-1.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip install text_generation==0.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2a88eb8e-2872-4065-a851-1f3fa7048f77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:52:57.518665Z",
     "iopub.status.busy": "2023-10-16T10:52:57.518256Z",
     "iopub.status.idle": "2023-10-16T10:52:57.523704Z",
     "shell.execute_reply": "2023-10-16T10:52:57.523198Z",
     "shell.execute_reply.started": "2023-10-16T10:52:57.518644Z"
    }
   },
   "outputs": [],
   "source": [
    "from text_generation import Client\n",
    "\n",
    "SERVER_PORT = 443\n",
    "SERVER_HOST = \"localhost\"\n",
    "SERVER_URL = f\"{SERVER_HOST}:{SERVER_PORT}\"\n",
    "tgi_client = Client(f\"http://{SERVER_URL}\", timeout=60)\n",
    "\n",
    "def invoke_tgi(prompt, \n",
    "                      random_seed=1, \n",
    "                      max_new_tokens=400, \n",
    "                      print_stream=True,\n",
    "                      assist_role=True):\n",
    "    if (assist_role):\n",
    "        prompt = f\"<|prompter|>{prompt}</s><|assistant|>\"\n",
    "    output = \"\"\n",
    "    for response in tgi_client.generate_stream(\n",
    "        prompt,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        return_full_text=False,\n",
    "        #temperature=None,\n",
    "        #truncate=None,\n",
    "        #seed=random_seed,\n",
    "        #typical_p=0.2,\n",
    "    ):\n",
    "        if hasattr(response, \"token\"):\n",
    "            if not response.token.special:\n",
    "                snippet = response.token.text\n",
    "                output += snippet\n",
    "                if (print_stream):\n",
    "                    print(snippet, end='', flush=True)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "68cd018a-ae4d-42ce-8dc0-b77d53afd92a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:53:19.032634Z",
     "iopub.status.busy": "2023-10-16T10:53:19.032262Z",
     "iopub.status.idle": "2023-10-16T10:53:31.428470Z",
     "shell.execute_reply": "2023-10-16T10:53:31.427823Z",
     "shell.execute_reply.started": "2023-10-16T10:53:19.032614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There are several challenges to supporting a long context for LLMs, including:\n",
      "\n",
      "1. Memory Limitations: LLMs are limited in the amount of data they can store in memory, which can limit the length of the context they can support.\n",
      "\n",
      "2. Computational Complexity: Generating responses for long contexts can be computationally expensive, especially for larger models.\n",
      "\n",
      "3. Training Data: Training data for long contexts can be difficult to obtain, as it requires a large amount of text data that is relevant and coherent.\n",
      "\n",
      "4. Bias and Unanswerable Questions: Long contexts can increase the likelihood of bias and unanswerable questions, as the model may not have seen similar contexts during training.\n",
      "\n",
      "5. Incomplete or Inconsistent Data: Long contexts can be more susceptible to errors and inconsistencies in the data, which can affect the accuracy of the model's responses.\n",
      "\n",
      "6. Maintenance and Updating: Maintaining and updating a long context can be challenging, as it requires regular monitoring and updating of the data to ensure its accuracy and relevance.\n",
      "\n",
      "7. User Interface: Supporting a long context may require a more complex user interface, which can be difficult to design and implement.\n",
      "\n",
      "8. Scalability: As the context grows, the model may become more difficult to scale, which can affect its performance and efficiency.\n",
      "\n",
      "9. Security and Privacy: Long contexts may raise concerns about security and privacy, as they may contain sensitive information that needs to be protected.\n",
      "\n",
      "10. Legal and Ethical Considerations: Long contexts may raise legal and ethical concerns, such as liability for the model's responses and the potential for bias in the data."
     ]
    }
   ],
   "source": [
    "prompt = \"What are the main challenges to support a long context for LLM?\"\n",
    "result = invoke_tgi(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "6581f69f-610e-42a3-a621-30fc83ae4e1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T10:55:26.173013Z",
     "iopub.status.busy": "2023-10-16T10:55:26.172661Z",
     "iopub.status.idle": "2023-10-16T10:55:26.476367Z",
     "shell.execute_reply": "2023-10-16T10:55:26.475476Z",
     "shell.execute_reply.started": "2023-10-16T10:55:26.172994Z"
    }
   },
   "outputs": [
    {
     "ename": "GenerationError",
     "evalue": "Request failed during generation: Server error: Out of available cache blocks: asked 946, only 154 free blocks",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/text_generation/client.py:259\u001b[0m, in \u001b[0;36mClient.generate_stream\u001b[0;34m(self, prompt, do_sample, max_new_tokens, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_p, truncate, typical_p, watermark, top_n_tokens)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 259\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mStreamResponse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mjson_payload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError:\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# If we failed to parse the payload, then it is an error payload\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/pydantic/main.py:150\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    149\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m \u001b[43m__pydantic_self__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__pydantic_self__\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for StreamResponse\ntoken\n  Field required [type=missing, input_value={'error': 'Request failed...ror_type': 'generation'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.0.3/v/missing",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mGenerationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[163], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m     task_instruction \u001b[38;5;241m=\u001b[39m task_instruction\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m      4\u001b[0m         my_question\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease tell me how does pgvector help with Generative AI and give me some examples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m     )\n\u001b[1;32m      6\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|prompter|>\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_instruction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m</s><|assistant|>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43minvoke_tgi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[157], line 16\u001b[0m, in \u001b[0;36minvoke_tgi\u001b[0;34m(prompt, random_seed, max_new_tokens, print_stream, assist_role)\u001b[0m\n\u001b[1;32m     14\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|prompter|>\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m</s><|assistant|>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m tgi_client\u001b[38;5;241m.\u001b[39mgenerate_stream(\n\u001b[1;32m     17\u001b[0m     prompt,\n\u001b[1;32m     18\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39mmax_new_tokens,\n\u001b[1;32m     20\u001b[0m     return_full_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m#temperature=None,\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m#truncate=None,\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m#seed=random_seed,\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m#typical_p=0.2,\u001b[39;00m\n\u001b[1;32m     25\u001b[0m ):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtoken\u001b[38;5;241m.\u001b[39mspecial:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/text_generation/client.py:262\u001b[0m, in \u001b[0;36mClient.generate_stream\u001b[0;34m(self, prompt, do_sample, max_new_tokens, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_p, truncate, typical_p, watermark, top_n_tokens)\u001b[0m\n\u001b[1;32m    259\u001b[0m     response \u001b[38;5;241m=\u001b[39m StreamResponse(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mjson_payload)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError:\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# If we failed to parse the payload, then it is an error payload\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m parse_error(resp\u001b[38;5;241m.\u001b[39mstatus_code, json_payload)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "\u001b[0;31mGenerationError\u001b[0m: Request failed during generation: Server error: Out of available cache blocks: asked 946, only 154 free blocks"
     ]
    }
   ],
   "source": [
    "with open(\"../example_long_ctx.txt\", \"r\") as fin:\n",
    "    task_instruction = fin.read()\n",
    "    task_instruction = task_instruction.format(\n",
    "        my_question=\"please tell me how does pgvector help with Generative AI and give me some examples.\"\n",
    "    )\n",
    "prompt = f\"<|prompter|>{task_instruction}</s><|assistant|>\"\n",
    "result = invoke_tgi(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
