{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87c4bfe1-272c-4189-bd13-a73aa9f81475",
   "metadata": {},
   "source": [
    "# How to Serve MistralFlite on TGI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8144f6b-d4cf-4868-92ba-a9d959fb5ad2",
   "metadata": {},
   "source": [
    "## Start TGI Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89423632-5115-46e8-83ce-310be9dc4fa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-17T02:59:16.533790Z",
     "iopub.status.busy": "2023-10-17T02:59:16.533077Z",
     "iopub.status.idle": "2023-10-17T02:59:16.648154Z",
     "shell.execute_reply": "2023-10-17T02:59:16.647485Z",
     "shell.execute_reply.started": "2023-10-17T02:59:16.533764Z"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65875577-1d2a-4cb7-90ed-b14088902e0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-17T03:00:48.601574Z",
     "iopub.status.busy": "2023-10-17T03:00:48.601037Z",
     "iopub.status.idle": "2023-10-17T03:00:57.156308Z",
     "shell.execute_reply": "2023-10-17T03:00:57.155519Z",
     "shell.execute_reply.started": "2023-10-17T03:00:48.601556Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  79.87kB\n",
      "Step 1/28 : FROM debian:bullseye-slim as pytorch-install\n",
      " ---> 1411dd05edfc\n",
      "Step 2/28 : ARG PYTORCH_VERSION=2.0.1\n",
      " ---> Using cache\n",
      " ---> ce0b1c250647\n",
      "Step 3/28 : ARG PYTHON_VERSION=3.9\n",
      " ---> Using cache\n",
      " ---> 31613d3e3dae\n",
      "Step 4/28 : ARG CUDA_VERSION=11.8\n",
      " ---> Using cache\n",
      " ---> a6c754f560da\n",
      "Step 5/28 : ARG MAMBA_VERSION=23.1.0-1\n",
      " ---> Using cache\n",
      " ---> 595317c72a6c\n",
      "Step 6/28 : ARG CUDA_CHANNEL=nvidia\n",
      " ---> Using cache\n",
      " ---> 4f0012d80a92\n",
      "Step 7/28 : ARG INSTALL_CHANNEL=pytorch\n",
      " ---> Using cache\n",
      " ---> 424e31a0e8cb\n",
      "Step 8/28 : ARG TARGETPLATFORM\n",
      " ---> Using cache\n",
      " ---> 23a82f9ff2ee\n",
      "Step 9/28 : ENV PATH /opt/conda/bin:$PATH\n",
      " ---> Using cache\n",
      " ---> 951b4363dcfa\n",
      "Step 10/28 : RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends         build-essential         ca-certificates         ccache         curl         git &&         rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 858c5d97d790\n",
      "Step 11/28 : RUN case ${TARGETPLATFORM} in          \"linux/arm64\")  MAMBA_ARCH=aarch64  ;;          *)              MAMBA_ARCH=x86_64   ;;     esac &&     curl -fsSL -v -o ~/mambaforge.sh -O  \"https://github.com/conda-forge/miniforge/releases/download/${MAMBA_VERSION}/Mambaforge-${MAMBA_VERSION}-Linux-${MAMBA_ARCH}.sh\"\n",
      " ---> Using cache\n",
      " ---> 43dacd9d332e\n",
      "Step 12/28 : RUN chmod +x ~/mambaforge.sh &&     bash ~/mambaforge.sh -b -p /opt/conda &&     rm ~/mambaforge.sh\n",
      " ---> Using cache\n",
      " ---> 26daf7dc76e9\n",
      "Step 13/28 : RUN case ${TARGETPLATFORM} in          \"linux/arm64\")  exit 1 ;;          *)              /opt/conda/bin/conda update -y conda &&                           /opt/conda/bin/conda install -c \"${INSTALL_CHANNEL}\" -c \"${CUDA_CHANNEL}\" -y \"python=${PYTHON_VERSION}\" pytorch==$PYTORCH_VERSION \"pytorch-cuda=$(echo $CUDA_VERSION | cut -d'.' -f 1-2)\"  ;;     esac &&     /opt/conda/bin/conda clean -ya\n",
      " ---> Using cache\n",
      " ---> fe87f251fc19\n",
      "Step 14/28 : FROM pytorch-install as kernel-builder\n",
      " ---> fe87f251fc19\n",
      "Step 15/28 : RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends         ninja-build         && rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> ad4b2c983215\n",
      "Step 16/28 : RUN /opt/conda/bin/conda install -c \"nvidia/label/cuda-11.8.0\"  cuda==11.8 &&     /opt/conda/bin/conda clean -ya\n",
      " ---> Using cache\n",
      " ---> a56f5e441b5d\n",
      "Step 17/28 : WORKDIR /usr/src\n",
      " ---> Using cache\n",
      " ---> 6b104a92b656\n",
      "Step 18/28 : RUN git clone https://github.com/OlivierDehaene/vllm.git\n",
      " ---> Using cache\n",
      " ---> 4c33e3c1a088\n",
      "Step 19/28 : WORKDIR /usr/src/vllm\n",
      " ---> Using cache\n",
      " ---> 88fd283fb0c7\n",
      "Step 20/28 : RUN git fetch\n",
      " ---> Using cache\n",
      " ---> 158977d4f6d6\n",
      "Step 21/28 : RUN git checkout d284b831c17f42a8ea63369a06138325f73c4cf9\n",
      " ---> Using cache\n",
      " ---> 02b486518eb9\n",
      "Step 22/28 : COPY vllm/attention_kernels.cu /usr/src/vllm/csrc/attention/attention_kernels.cu\n",
      " ---> Using cache\n",
      " ---> c681f3c3d740\n",
      "Step 23/28 : COPY vllm/test_attention.py /usr/src/vllm/tests/kernels/test_attention.py\n",
      " ---> Using cache\n",
      " ---> b4fb1983fab3\n",
      "Step 24/28 : COPY vllm//utils.py  /usr/src/vllm/vllm/utils.py\n",
      " ---> Using cache\n",
      " ---> a8957bec0db8\n",
      "Step 25/28 : COPY vllm/worker.py /usr/src/vllm/vllm/worker/worker.py\n",
      " ---> Using cache\n",
      " ---> 2616ada3da62\n",
      "Step 26/28 : RUN python setup.py build\n",
      " ---> Using cache\n",
      " ---> 8c273a923ae3\n",
      "Step 27/28 : FROM ghcr.io/huggingface/text-generation-inference:1.1.0\n",
      " ---> a111faa3a21b\n",
      "Step 28/28 : COPY --from=kernel-builder /usr/src/vllm/build/lib.linux-x86_64-cpython-39 /opt/conda/lib/python3.9/site-packages\n",
      " ---> 7340c352527c\n",
      "Successfully built 7340c352527c\n",
      "Successfully tagged mistrallite:1.1.0\n"
     ]
    }
   ],
   "source": [
    "!bash ./docker_build.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a07043b7-43c3-41a3-a9e0-3049834b32ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-17T03:01:05.492144Z",
     "iopub.status.busy": "2023-10-17T03:01:05.491741Z",
     "iopub.status.idle": "2023-10-17T03:01:06.554562Z",
     "shell.execute_reply": "2023-10-17T03:01:06.553928Z",
     "shell.execute_reply.started": "2023-10-17T03:01:05.492126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a6f065db42eddb6e27879abc8e5a11845fe0658295853fda17efd4c4af3846e1\n"
     ]
    }
   ],
   "source": [
    "!bash start_mistrallite.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b15f0e-926f-4c3d-b648-d1de85b0a82b",
   "metadata": {},
   "source": [
    "> Warning: You may need to wait for 10+ minutes for the docker container to be ready for the first time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57d026d-c5e1-45d9-8104-adb70f9ca890",
   "metadata": {},
   "source": [
    "## Perform Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65a91fc8-3b33-47a0-98c3-95415168b9ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T04:52:08.159446Z",
     "iopub.status.busy": "2023-10-16T04:52:08.159015Z",
     "iopub.status.idle": "2023-10-16T04:52:12.969501Z",
     "shell.execute_reply": "2023-10-16T04:52:12.968896Z",
     "shell.execute_reply.started": "2023-10-16T04:52:08.159427Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting text_generation\n",
      "  Obtaining dependency information for text_generation from https://files.pythonhosted.org/packages/14/f7/cadf3a0fc619a72d7c667d16e96ef0a5b4c557e6e2b4788a0360dfba4fee/text_generation-0.6.1-py3-none-any.whl.metadata\n",
      "  Downloading text_generation-0.6.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting aiohttp<4.0,>=3.8 (from text_generation)\n",
      "  Obtaining dependency information for aiohttp<4.0,>=3.8 from https://files.pythonhosted.org/packages/41/8e/4c48881316bbced3d13089c4d0df4be321ce79a0c695d82dee9996aaf56b/aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from text_generation) (0.16.4)\n",
      "Requirement already satisfied: pydantic<3,>1.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from text_generation) (2.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->text_generation) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->text_generation) (3.2.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0,>=3.8->text_generation)\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp<4.0,>=3.8->text_generation)\n",
      "  Obtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0,>=3.8->text_generation)\n",
      "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp<4.0,>=3.8->text_generation)\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/1e/28/74b8b6451c89c070d34e753d8b65a1e4ce508a6808b18529f36e8c0e2184/frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0,>=3.8->text_generation)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.12->text_generation) (3.12.2)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.12->text_generation) (2023.6.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.12->text_generation) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.12->text_generation) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.12->text_generation) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.12->text_generation) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.12->text_generation) (21.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3,>1.10->text_generation) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3,>1.10->text_generation) (2.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub<1.0,>=0.12->text_generation) (3.0.9)\n",
      "Requirement already satisfied: idna>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp<4.0,>=3.8->text_generation) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.12->text_generation) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.12->text_generation) (2023.5.7)\n",
      "Downloading text_generation-0.6.1-py3-none-any.whl (10 kB)\n",
      "Downloading aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.7/225.7 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, text_generation\n",
      "Successfully installed aiohttp-3.8.6 aiosignal-1.3.1 async-timeout-4.0.3 frozenlist-1.4.0 multidict-6.0.4 text_generation-0.6.1 yarl-1.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip install text_generation==0.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a88eb8e-2872-4065-a851-1f3fa7048f77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-17T03:15:09.420668Z",
     "iopub.status.busy": "2023-10-17T03:15:09.420256Z",
     "iopub.status.idle": "2023-10-17T03:15:09.941644Z",
     "shell.execute_reply": "2023-10-17T03:15:09.940957Z",
     "shell.execute_reply.started": "2023-10-17T03:15:09.420650Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/pydantic/_internal/_fields.py:126: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from text_generation import Client\n",
    "\n",
    "SERVER_PORT = 443\n",
    "SERVER_HOST = \"localhost\"\n",
    "SERVER_URL = f\"{SERVER_HOST}:{SERVER_PORT}\"\n",
    "tgi_client = Client(f\"http://{SERVER_URL}\", timeout=60)\n",
    "\n",
    "def invoke_tgi(prompt, \n",
    "                      random_seed=1, \n",
    "                      max_new_tokens=400, \n",
    "                      print_stream=True,\n",
    "                      assist_role=True):\n",
    "    if (assist_role):\n",
    "        prompt = f\"<|prompter|>{prompt}</s><|assistant|>\"\n",
    "    output = \"\"\n",
    "    for response in tgi_client.generate_stream(\n",
    "        prompt,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        return_full_text=False,\n",
    "        #temperature=None,\n",
    "        #truncate=None,\n",
    "        #seed=random_seed,\n",
    "        #typical_p=0.2,\n",
    "    ):\n",
    "        if hasattr(response, \"token\"):\n",
    "            if not response.token.special:\n",
    "                snippet = response.token.text\n",
    "                output += snippet\n",
    "                if (print_stream):\n",
    "                    print(snippet, end='', flush=True)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68cd018a-ae4d-42ce-8dc0-b77d53afd92a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-17T03:15:33.132755Z",
     "iopub.status.busy": "2023-10-17T03:15:33.132315Z",
     "iopub.status.idle": "2023-10-17T03:15:45.613599Z",
     "shell.execute_reply": "2023-10-17T03:15:45.613033Z",
     "shell.execute_reply.started": "2023-10-17T03:15:33.132737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There are several challenges to supporting a long context for LLMs, including:\n",
      "\n",
      "1. Memory Limitations: LLMs are limited in the amount of data they can store in memory, which can limit the length of the context they can support.\n",
      "\n",
      "2. Computational Complexity: Generating a response for a long context can be computationally expensive, especially for larger models.\n",
      "\n",
      "3. Training Data: Training an LLM to support a long context requires a large amount of training data, which can be difficult and time-consuming to collect.\n",
      "\n",
      "4. Bias and Unanswerable Questions: As the context gets longer, there is a greater risk of bias and unanswerable questions, which can be difficult for the model to handle.\n",
      "\n",
      "5. Human Evaluation: Evaluating the quality of responses for a long context can be challenging, as it may require human evaluators to read and understand a large amount of text.\n",
      "\n",
      "6. Scalability: As the context gets longer, it becomes more difficult to scale the model to handle the increased computational demands.\n",
      "\n",
      "7. Interpretability: It can be difficult to understand how an LLM arrives at its responses for a long context, making it challenging to identify and address any biases or errors in the model.\n",
      "\n",
      "8. Cost: Supporting a long context can be expensive, as it requires more computational resources and training data.\n",
      "\n",
      "9. Security and Privacy: As the context gets longer, there is a greater risk of security and privacy concerns, as the model may be exposed to sensitive information.\n",
      "\n",
      "10. Legal and Ethical Considerations: Supporting a long context may raise legal and ethical concerns, such as the potential for the model to generate biased or harmful responses."
     ]
    }
   ],
   "source": [
    "prompt = \"What are the main challenges to support a long context for LLM?\"\n",
    "result = invoke_tgi(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3036b280-723d-4f56-bea4-2557644cc64d",
   "metadata": {},
   "source": [
    "Try the long context of over 13,400 tokens, which are copied from [Amazon Aurora FAQs](https://aws.amazon.com/rds/aurora/faqs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ddd4c04-5f53-494e-a4c8-f32c01e8a58e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-17T03:16:06.922101Z",
     "iopub.status.busy": "2023-10-17T03:16:06.921627Z",
     "iopub.status.idle": "2023-10-17T03:16:29.016372Z",
     "shell.execute_reply": "2023-10-17T03:16:29.015626Z",
     "shell.execute_reply.started": "2023-10-17T03:16:06.922082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pgvector is an open-source extension for PostgreSQL supported by Amazon Aurora PostgreSQL-Compatible Edition.\n",
      "\n",
      "You can use pgvector to store, search, index, and query billions of embeddings that are generated from machine learning (ML) and artificial intelligence (AI) models in your database, such as those from Amazon Bedrock (limited preview) or Amazon SageMaker. A vector embedding is a numerical representation that represents the semantic meaning of content such as text, images, and video.\n",
      "\n",
      "With pgvector, you can query embeddings in your Aurora PostgreSQL database to perform efficient semantic similarity searches of these data types, represented as vectors, combined with other tabular data in Aurora. This enables the use of generative AI and other AI/ML systems for new types of applications such as personalized recommendations based on similar text descriptions or images, candidate match based on interview notes, customer service next best action recommendations based on successful transcripts or chat session dialogs, and more."
     ]
    }
   ],
   "source": [
    "with open(\"../example_long_ctx.txt\", \"r\") as fin:\n",
    "    task_instruction = fin.read()\n",
    "    task_instruction = task_instruction.format(\n",
    "        my_question=\"please tell me how does pgvector help with Generative AI and give me some examples.\"\n",
    "    )\n",
    "prompt = f\"<|prompter|>{task_instruction}</s><|assistant|>\"\n",
    "result = invoke_tgi(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
