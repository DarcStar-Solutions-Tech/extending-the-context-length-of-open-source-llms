{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87c4bfe1-272c-4189-bd13-a73aa9f81475",
   "metadata": {},
   "source": [
    "# Serving MistralLite on HuggingFace Text Generation Inference Container\n",
    "\n",
    "This notebook provides a step-by-step walkthrough of deploying the open source MistralLite model for natural language generation with HuggingFace's [Text Generation Inference Container](https://github.com/huggingface/text-generation-inference). In this notebook, we will deploy the container for LLM inference, and invoke the deployed endpoint with example prompts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8144f6b-d4cf-4868-92ba-a9d959fb5ad2",
   "metadata": {},
   "source": [
    "## Start TGI Server\n",
    "\n",
    "This repository has bash scripts to download the model, and prepare and deploy the Docker container with the LLM for inference. Execute the following cells to run pre-written bash scripts to configure the Docker container and launch the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89423632-5115-46e8-83ce-310be9dc4fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65875577-1d2a-4cb7-90ed-b14088902e0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!bash ./docker_build.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07043b7-43c3-41a3-a9e0-3049834b32ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash start_mistrallite.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b15f0e-926f-4c3d-b648-d1de85b0a82b",
   "metadata": {},
   "source": [
    "> Warning: You may need to wait for 10+ minutes for the docker container to be ready for the first time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57d026d-c5e1-45d9-8104-adb70f9ca890",
   "metadata": {},
   "source": [
    "## Perform Inference\n",
    "\n",
    "We can now invoke the model by first installing the `text-generation` library and define an invocation function to prompt the deployed model. Example prompts including a long-context prompt are included to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a91fc8-3b33-47a0-98c3-95415168b9ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install text_generation==0.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a88eb8e-2872-4065-a851-1f3fa7048f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_generation import Client\n",
    "\n",
    "SERVER_PORT = 443\n",
    "SERVER_HOST = \"localhost\"\n",
    "SERVER_URL = f\"{SERVER_HOST}:{SERVER_PORT}\"\n",
    "tgi_client = Client(f\"http://{SERVER_URL}\", timeout=60)\n",
    "\n",
    "def invoke_tgi(prompt, \n",
    "                      random_seed=1, \n",
    "                      max_new_tokens=400, \n",
    "                      print_stream=True,\n",
    "                      assist_role=True):\n",
    "    if (assist_role):\n",
    "        prompt = f\"<|prompter|>{prompt}</s><|assistant|>\"\n",
    "    output = \"\"\n",
    "    for response in tgi_client.generate_stream(\n",
    "        prompt,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        return_full_text=False,\n",
    "        #temperature=None,\n",
    "        #truncate=None,\n",
    "        #seed=random_seed,\n",
    "        #typical_p=0.2,\n",
    "    ):\n",
    "        if hasattr(response, \"token\"):\n",
    "            if not response.token.special:\n",
    "                snippet = response.token.text\n",
    "                output += snippet\n",
    "                if (print_stream):\n",
    "                    print(snippet, end='', flush=True)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cd018a-ae4d-42ce-8dc0-b77d53afd92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What are the main challenges to support a long context for LLM?\"\n",
    "result = invoke_tgi(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3036b280-723d-4f56-bea4-2557644cc64d",
   "metadata": {},
   "source": [
    "Try the long context of over 13,400 tokens, which are copied from [Amazon Aurora FAQs](https://aws.amazon.com/rds/aurora/faqs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddd4c04-5f53-494e-a4c8-f32c01e8a58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../example_long_ctx.txt\", \"r\") as fin:\n",
    "    task_instruction = fin.read()\n",
    "    task_instruction = task_instruction.format(\n",
    "        my_question=\"please tell me how does pgvector help with Generative AI and give me some examples.\"\n",
    "    )\n",
    "prompt = f\"<|prompter|>{task_instruction}</s><|assistant|>\"\n",
    "result = invoke_tgi(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
