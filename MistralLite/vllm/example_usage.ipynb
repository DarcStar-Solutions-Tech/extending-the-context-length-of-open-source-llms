{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0b69c5-4260-4eb0-8b2d-733923508a1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!bash ./start_vllm_container.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bf0a6b-d184-4991-a822-92b5c7c2a49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai==0.28.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815bdd35-ae92-4ccb-9f9b-55dda5d7fb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def invoke_vllm(prompt, assist_role=True, stream=True, max_tokens=400):\n",
    "\n",
    "    # Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "    openai.api_key = \"EMPTY\"\n",
    "    vllm_container_port = 8000\n",
    "    openai.api_base = f\"http://localhost:{vllm_container_port}/v1\"\n",
    "\n",
    "    # List models API\n",
    "    models = openai.Model.list()\n",
    "    #print(\"Models:\", models)\n",
    "\n",
    "    model = models[\"data\"][0][\"id\"]\n",
    "\n",
    "    # Completion API\n",
    "    \n",
    "    if (assist_role):\n",
    "        prompt = f\"<|prompter|>{prompt}</s><|assistant|>\"\n",
    "\n",
    "    completion = openai.Completion.create(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        echo=False,\n",
    "        n=1,\n",
    "        stream=stream,\n",
    "        max_tokens=max_tokens,\n",
    "        #ignore_eos=False,\n",
    "        #temperature=0.1,\n",
    "        temperature=0,\n",
    "        #top_p=0.95,\n",
    "        #stop=[\"</s>\"],\n",
    "     )\n",
    "    if stream:\n",
    "        output = \"\"\n",
    "        for c in completion:\n",
    "            #print(c)\n",
    "            d = c[\"choices\"][0][\"text\"]\n",
    "            output += d\n",
    "            print(d, end='', flush=True)      \n",
    "    else:\n",
    "        output = completion[\"choices\"][0][\"text\"]\n",
    "        print(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cead05-0bc7-4b35-8edc-72d69c889368",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What are the main challenges to support a long context for LLM?\"\n",
    "result = invoke_vllm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c9e784-3cc3-4e94-8ced-3195c38d0d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../example_long_ctx.txt\", \"r\") as fin:\n",
    "    task_instruction = fin.read()\n",
    "    task_instruction = task_instruction.format(\n",
    "        my_question=\"please tell me how does pgvector help with Generative AI and give me some examples.\"\n",
    "    )\n",
    "prompt = f\"<|prompter|>{task_instruction}</s><|assistant|>\"\n",
    "invoke_vllm(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
