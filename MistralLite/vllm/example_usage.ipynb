{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9102a37",
   "metadata": {},
   "source": [
    "# Using MistralLite with vLLM Server\n",
    "\n",
    "This notebook provides a step-by-step walkthrough of using the open source MistralLite model for natural language generation with [vLLM](https://vllm.readthedocs.io/en/latest/index.html). We will initialize the model serving container using bash scripts, and perform inference using [OpenAI-compatible serving](https://vllm.readthedocs.io/en/latest/getting_started/quickstart.html#openai-compatible-server). The notebook includes code to load and start the model container, test it on short prompts, and demonstrate its capabilities on longer prompts from files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0b69c5-4260-4eb0-8b2d-733923508a1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!bash ./start_vllm_container.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bf0a6b-d184-4991-a822-92b5c7c2a49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai==0.28.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41bdec7",
   "metadata": {},
   "source": [
    "# Invoking the vLLM Container\n",
    "\n",
    "Start invoking the deployed MistralLite model by replacing the OpenAI API key with your API key, and execute the following cells to query with example prompts with and without additional context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815bdd35-ae92-4ccb-9f9b-55dda5d7fb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def invoke_vllm(prompt, assist_role=True, stream=True, max_tokens=400):\n",
    "\n",
    "    # Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "    openai.api_key = \"EMPTY\"\n",
    "    vllm_container_port = 8000\n",
    "    openai.api_base = f\"http://localhost:{vllm_container_port}/v1\"\n",
    "\n",
    "    # List models API\n",
    "    models = openai.Model.list()\n",
    "    #print(\"Models:\", models)\n",
    "\n",
    "    model = models[\"data\"][0][\"id\"]\n",
    "\n",
    "    # Completion API\n",
    "    \n",
    "    if (assist_role):\n",
    "        prompt = f\"<|prompter|>{prompt}</s><|assistant|>\"\n",
    "\n",
    "    completion = openai.Completion.create(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        echo=False,\n",
    "        n=1,\n",
    "        stream=stream,\n",
    "        max_tokens=max_tokens,\n",
    "        #ignore_eos=False,\n",
    "        #temperature=0.1,\n",
    "        temperature=0,\n",
    "        #top_p=0.95,\n",
    "        #stop=[\"</s>\"],\n",
    "     )\n",
    "    if stream:\n",
    "        output = \"\"\n",
    "        for c in completion:\n",
    "            #print(c)\n",
    "            d = c[\"choices\"][0][\"text\"]\n",
    "            output += d\n",
    "            print(d, end='', flush=True)      \n",
    "    else:\n",
    "        output = completion[\"choices\"][0][\"text\"]\n",
    "        print(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cead05-0bc7-4b35-8edc-72d69c889368",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What are the main challenges to support a long context for LLM?\"\n",
    "result = invoke_vllm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c9e784-3cc3-4e94-8ced-3195c38d0d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../example_long_ctx.txt\", \"r\") as fin:\n",
    "    task_instruction = fin.read()\n",
    "    task_instruction = task_instruction.format(\n",
    "        my_question=\"please tell me how does pgvector help with Generative AI and give me some examples.\"\n",
    "    )\n",
    "prompt = f\"<|prompter|>{task_instruction}</s><|assistant|>\"\n",
    "invoke_vllm(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
