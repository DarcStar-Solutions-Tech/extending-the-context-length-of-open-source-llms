{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b0b69c5-4260-4eb0-8b2d-733923508a1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T23:37:16.121799Z",
     "iopub.status.busy": "2023-10-16T23:37:16.121453Z",
     "iopub.status.idle": "2023-10-16T23:43:00.884668Z",
     "shell.execute_reply": "2023-10-16T23:43:00.884046Z",
     "shell.execute_reply.started": "2023-10-16T23:37:16.121781Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "682fed21b0c09df2ae8d8afd84987a8156d9204dbf35782f333ebdc222aa8980\n",
      "docker: Error response from daemon: driver failed programming external connectivity on endpoint inspiring_stonebraker (1e38955ba096c4fc0c021e3ece3f3ab326638d8f235c332a1b10cadcf9e582d2): Bind for 0.0.0.0:8000 failed: port is already allocated.\n"
     ]
    }
   ],
   "source": [
    "!bash ./start_vllm_container.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01bf0a6b-d184-4991-a822-92b5c7c2a49f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T23:44:54.495639Z",
     "iopub.status.busy": "2023-10-16T23:44:54.495275Z",
     "iopub.status.idle": "2023-10-16T23:44:57.585536Z",
     "shell.execute_reply": "2023-10-16T23:44:57.584798Z",
     "shell.execute_reply.started": "2023-10-16T23:44:54.495622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Obtaining dependency information for openai from https://files.pythonhosted.org/packages/1e/9f/385c25502f437686e4aa715969e5eaf5c2cb5e5ffa7c5cdd52f3c6ae967a/openai-0.28.1-py3-none-any.whl.metadata\n",
      "  Downloading openai-0.28.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: requests>=2.20 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from openai) (3.8.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.20->openai) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.20->openai) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.20->openai) (2023.5.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->openai) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: openai\n",
      "Successfully installed openai-0.28.1\n"
     ]
    }
   ],
   "source": [
    "!pip install openai==0.28.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "815bdd35-ae92-4ccb-9f9b-55dda5d7fb1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-17T01:32:38.337472Z",
     "iopub.status.busy": "2023-10-17T01:32:38.337009Z",
     "iopub.status.idle": "2023-10-17T01:32:38.342592Z",
     "shell.execute_reply": "2023-10-17T01:32:38.341943Z",
     "shell.execute_reply.started": "2023-10-17T01:32:38.337455Z"
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def invoke_vllm(prompt, assist_role=True, stream=True, max_tokens=400):\n",
    "\n",
    "    # Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "    openai.api_key = \"EMPTY\"\n",
    "    vllm_container_port = 8000\n",
    "    openai.api_base = f\"http://localhost:{vllm_container_port}/v1\"\n",
    "\n",
    "    # List models API\n",
    "    models = openai.Model.list()\n",
    "    #print(\"Models:\", models)\n",
    "\n",
    "    model = models[\"data\"][0][\"id\"]\n",
    "\n",
    "    # Completion API\n",
    "    \n",
    "    if (assist_role):\n",
    "        prompt = f\"<|prompter|>{prompt}</s><|assistant|>\"\n",
    "\n",
    "    completion = openai.Completion.create(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        echo=False,\n",
    "        n=1,\n",
    "        stream=stream,\n",
    "        max_tokens=max_tokens,\n",
    "        #ignore_eos=False,\n",
    "        #temperature=0.1,\n",
    "        temperature=0,\n",
    "        #top_p=0.95,\n",
    "        #stop=[\"</s>\"],\n",
    "     )\n",
    "    if stream:\n",
    "        output = \"\"\n",
    "        for c in completion:\n",
    "            #print(c)\n",
    "            d = c[\"choices\"][0][\"text\"]\n",
    "            output += d\n",
    "            print(d, end='', flush=True)      \n",
    "    else:\n",
    "        output = completion[\"choices\"][0][\"text\"]\n",
    "        print(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88cead05-0bc7-4b35-8edc-72d69c889368",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-17T01:32:40.539478Z",
     "iopub.status.busy": "2023-10-17T01:32:40.538748Z",
     "iopub.status.idle": "2023-10-17T01:32:53.894551Z",
     "shell.execute_reply": "2023-10-17T01:32:53.894029Z",
     "shell.execute_reply.started": "2023-10-17T01:32:40.539456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There are several challenges to supporting a long context for LLMs, including:\n",
      "\n",
      "1. Memory Limitations: LLMs are limited in the amount of data they can store in memory, which can limit the length of the context they can support.\n",
      "\n",
      "2. Computational Complexity: Generating responses for long contexts can be computationally expensive, especially for larger models.\n",
      "\n",
      "3. Training Data: Training data for long contexts can be difficult to obtain, as it requires a large amount of text data that is relevant and coherent.\n",
      "\n",
      "4. Bias and Unanswerable Questions: Long contexts can increase the likelihood of bias and unanswerable questions, as the model may not have seen similar contexts during training.\n",
      "\n",
      "5. Incomplete or Inconsistent Data: Long contexts can be more susceptible to errors and inconsistencies in the data, which can affect the accuracy of the model's responses.\n",
      "\n",
      "6. Maintenance and Updating: Maintaining and updating a long context can be challenging, as it requires regular monitoring and updating of the data to ensure its accuracy and relevance.\n",
      "\n",
      "7. User Interface: Supporting a long context may require a more complex user interface, which can be difficult to design and implement.\n",
      "\n",
      "8. Security and Privacy: Long contexts can raise concerns about security and privacy, as they may contain sensitive information that needs to be protected.\n",
      "\n",
      "9. Legal and Ethical Considerations: Long contexts may raise legal and ethical concerns, such as liability for the model's responses or the potential for bias in the data.\n",
      "\n",
      "10. Cost: Supporting a long context can be expensive, as it requires significant computational resources and may require additional training data."
     ]
    }
   ],
   "source": [
    "prompt = \"What are the main challenges to support a long context for LLM?\"\n",
    "result = invoke_vllm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54c9e784-3cc3-4e94-8ced-3195c38d0d58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-17T01:32:56.381791Z",
     "iopub.status.busy": "2023-10-17T01:32:56.381417Z",
     "iopub.status.idle": "2023-10-17T01:33:11.987325Z",
     "shell.execute_reply": "2023-10-17T01:33:11.986784Z",
     "shell.execute_reply.started": "2023-10-17T01:32:56.381774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pgvector is an open-source extension for PostgreSQL supported by Amazon Aurora PostgreSQL-Compatible Edition.\n",
      "\n",
      "You can use pgvector to store, search, index, and query billions of embeddings that are generated from machine learning (ML) and artificial intelligence (AI) models in your database, such as those from Amazon Bedrock (limited preview) or Amazon SageMaker. A vector embedding is a numerical representation that represents the semantic meaning of content such as text, images, and video.\n",
      "\n",
      "With pgvector, you can query embeddings in your Aurora PostgreSQL database to perform efficient semantic similarity searches of these data types, represented as vectors, combined with other tabular data in Aurora. This enables the use of generative AI and other AI/ML systems for new types of applications such as personalized recommendations based on similar text descriptions or images, candidate match based on interview notes, customer service next best action recommendations based on successful transcripts or chat session dialogs, and more."
     ]
    },
    {
     "data": {
      "text/plain": [
       "' pgvector is an open-source extension for PostgreSQL supported by Amazon Aurora PostgreSQL-Compatible Edition.\\n\\nYou can use pgvector to store, search, index, and query billions of embeddings that are generated from machine learning (ML) and artificial intelligence (AI) models in your database, such as those from Amazon Bedrock (limited preview) or Amazon SageMaker. A vector embedding is a numerical representation that represents the semantic meaning of content such as text, images, and video.\\n\\nWith pgvector, you can query embeddings in your Aurora PostgreSQL database to perform efficient semantic similarity searches of these data types, represented as vectors, combined with other tabular data in Aurora. This enables the use of generative AI and other AI/ML systems for new types of applications such as personalized recommendations based on similar text descriptions or images, candidate match based on interview notes, customer service next best action recommendations based on successful transcripts or chat session dialogs, and more.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../example_long_ctx.txt\", \"r\") as fin:\n",
    "    task_instruction = fin.read()\n",
    "    task_instruction = task_instruction.format(\n",
    "        my_question=\"please tell me how does pgvector help with Generative AI and give me some examples.\"\n",
    "    )\n",
    "prompt = f\"<|prompter|>{task_instruction}</s><|assistant|>\"\n",
    "invoke_vllm(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
